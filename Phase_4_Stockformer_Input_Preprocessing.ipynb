{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b56065e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node2vec is already installed\n",
      "Libraries loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Install and import graph embedding libraries\n",
    "try:\n",
    "    import node2vec\n",
    "    print(\"node2vec is already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing node2vec and dependencies...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'node2vec', 'networkx', 'gensim', '-q'])\n",
    "    print(\"Installation complete!\")\n",
    "    import node2vec\n",
    "\n",
    "import networkx as nx\n",
    "from node2vec import Node2Vec\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f30bb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory: /home/ubuntu/rajnish/Multitask-Stockformer\n",
      "Factor directory: /home/ubuntu/rajnish/Multitask-Stockformer/data/NIFTY200/Alpha_158_2022-01-01_2024-08-31\n",
      "Label file: /home/ubuntu/rajnish/Multitask-Stockformer/data/NIFTY200/Stock_NIFTY_2022-01-01_2024-08-31/label.csv\n",
      "Output directory: /home/ubuntu/rajnish/Multitask-Stockformer/data/NIFTY200/Stock_NIFTY_2022-01-01_2024-08-31\n",
      "\n",
      "Wavelet: sym2, Level: 1\n",
      "Correlation threshold: 0.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywt  # PyWavelets for DWT\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "BASE_DIR = Path('/home/ubuntu/rajnish/Multitask-Stockformer')\n",
    "FACTOR_DIR = BASE_DIR / 'data/NIFTY200/Alpha_158_2022-01-01_2024-08-31'\n",
    "LABEL_FILE = BASE_DIR / 'data/NIFTY200/Stock_NIFTY_2022-01-01_2024-08-31/label.csv'\n",
    "OUTPUT_DIR = BASE_DIR / 'data/NIFTY200/Stock_NIFTY_2022-01-01_2024-08-31'\n",
    "\n",
    "# Wavelet parameters (matching original paper)\n",
    "WAVELET_TYPE = 'sym2'  # Symlet 2\n",
    "WAVELET_LEVEL = 1      # Single-level decomposition\n",
    "\n",
    "# Correlation threshold for adjacency matrix\n",
    "CORR_THRESHOLD = 0.3\n",
    "\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Factor directory: {FACTOR_DIR}\")\n",
    "print(f\"Label file: {LABEL_FILE}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"\\nWavelet: {WAVELET_TYPE}, Level: {WAVELET_LEVEL}\")\n",
    "print(f\"Correlation threshold: {CORR_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1316f3",
   "metadata": {},
   "source": [
    "## 1. Setup & Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b312039",
   "metadata": {},
   "source": [
    "# Phase 4: Stockformer Input Preprocessing\n",
    "\n",
    "Transform Phase 3 outputs (22 Alpha158 factors) into Stockformer model input files:\n",
    "1. `flow.npz` - Wavelet-decomposed factors\n",
    "2. `trend_indicator.npz` - Binary trend labels\n",
    "3. `corr_adj.npy` - Correlation adjacency matrix\n",
    "4. `128_corr_struc2vec_adjgat.npy` - Graph embeddings (MANDATORY)\n",
    "5. `label.csv` - Already exists from Phase 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2205c4de",
   "metadata": {},
   "source": [
    "## 2. Load Factor Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2a1c6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of factors: 22\n",
      "\n",
      "Factor list:\n",
      " 1. STD20\n",
      " 2. KLEN\n",
      " 3. BETA60\n",
      " 4. BETA20\n",
      " 5. HIGH0\n",
      " 6. STD10\n",
      " 7. MIN60\n",
      " 8. KUP\n",
      " 9. RESI20\n",
      "10. QTLD60\n",
      "11. BETA30\n",
      "12. STD30\n",
      "13. IMIN30\n",
      "14. MA60\n",
      "15. MAX5\n",
      "16. MIN30\n",
      "17. MIN20\n",
      "18. CORD30\n",
      "19. ROC30\n",
      "20. CORR60\n",
      "21. IMXD30\n",
      "22. STD5\n"
     ]
    }
   ],
   "source": [
    "# Load selected factor names\n",
    "factor_list_file = FACTOR_DIR / 'selected_factors.txt'\n",
    "with open(factor_list_file, 'r') as f:\n",
    "    selected_factors = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print(f\"Number of factors: {len(selected_factors)}\")\n",
    "print(f\"\\nFactor list:\")\n",
    "for i, factor in enumerate(selected_factors, 1):\n",
    "    print(f\"{i:2d}. {factor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f60e80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading factor CSVs...\n",
      "\n",
      "Loaded 22 factor files\n",
      "\n",
      "Data dimensions:\n",
      "  Time steps: 660\n",
      "  Stocks: 191\n",
      "  Factors: 22\n",
      "\n",
      "Date range: 2022-01-03 to 2024-08-30\n",
      "Sample stocks: ['360ONE', 'ABB', 'ABCAPITAL', 'ACC', 'ADANIENSOL']\n"
     ]
    }
   ],
   "source": [
    "# Load all factor CSVs into a 3D structure: [time, stock, factor]\n",
    "print(\"Loading factor CSVs...\")\n",
    "\n",
    "factor_dfs = {}\n",
    "for factor_name in selected_factors:\n",
    "    csv_path = FACTOR_DIR / f\"{factor_name}.csv\"\n",
    "    if csv_path.exists():\n",
    "        df = pd.read_csv(csv_path, index_col=0)  # index is date\n",
    "        factor_dfs[factor_name] = df\n",
    "    else:\n",
    "        print(f\"Warning: {csv_path} not found\")\n",
    "\n",
    "print(f\"\\nLoaded {len(factor_dfs)} factor files\")\n",
    "\n",
    "# Get dimensions\n",
    "first_factor = list(factor_dfs.values())[0]\n",
    "num_dates = len(first_factor)\n",
    "num_stocks = len(first_factor.columns)\n",
    "num_factors = len(factor_dfs)\n",
    "\n",
    "print(f\"\\nData dimensions:\")\n",
    "print(f\"  Time steps: {num_dates}\")\n",
    "print(f\"  Stocks: {num_stocks}\")\n",
    "print(f\"  Factors: {num_factors}\")\n",
    "print(f\"\\nDate range: {first_factor.index[0]} to {first_factor.index[-1]}\")\n",
    "print(f\"Sample stocks: {list(first_factor.columns[:5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a77bd6",
   "metadata": {},
   "source": [
    "## 3. Create 3D Factor Tensor\n",
    "\n",
    "Convert dictionary of DataFrames to numpy array: `[time, stock, factor]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96bf0235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factor tensor shape: (660, 191, 22)\n",
      "  [time=660, stock=191, factor=22]\n",
      "\n",
      "Tensor statistics:\n",
      "  Min: nan\n",
      "  Max: nan\n",
      "  Mean: nan\n",
      "  Std: nan\n",
      "  NaN count: 201078\n",
      "  Inf count: 0\n"
     ]
    }
   ],
   "source": [
    "# Create 3D tensor: [time, stock, factor]\n",
    "factor_tensor = np.zeros((num_dates, num_stocks, num_factors))\n",
    "\n",
    "for factor_idx, factor_name in enumerate(selected_factors):\n",
    "    if factor_name in factor_dfs:\n",
    "        factor_tensor[:, :, factor_idx] = factor_dfs[factor_name].values\n",
    "\n",
    "print(f\"Factor tensor shape: {factor_tensor.shape}\")\n",
    "print(f\"  [time={num_dates}, stock={num_stocks}, factor={num_factors}]\")\n",
    "print(f\"\\nTensor statistics:\")\n",
    "print(f\"  Min: {factor_tensor.min():.6f}\")\n",
    "print(f\"  Max: {factor_tensor.max():.6f}\")\n",
    "print(f\"  Mean: {factor_tensor.mean():.6f}\")\n",
    "print(f\"  Std: {factor_tensor.std():.6f}\")\n",
    "print(f\"  NaN count: {np.isnan(factor_tensor).sum()}\")\n",
    "print(f\"  Inf count: {np.isinf(factor_tensor).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56722bce",
   "metadata": {},
   "source": [
    "## 4. Wavelet Decomposition (DWT)\n",
    "\n",
    "Apply single-level Discrete Wavelet Transform (sym2) to decompose each factor time series into:\n",
    "- **Approximation coefficients (cA):** Low-frequency component (trend)\n",
    "- **Detail coefficients (cD):** High-frequency component (noise/volatility)\n",
    "\n",
    "Original paper uses this to separate slow-changing trends from rapid fluctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f21dd790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying sym2 wavelet decomposition (level=1)...\n",
      "Processing each stock-factor combination...\n",
      "\n",
      "  Processing stock 50/191...\n",
      "  Processing stock 100/191...\n",
      "  Processing stock 150/191...\n",
      "\n",
      "Wavelet decomposition complete!\n",
      "\n",
      "Low-frequency component shape: (331, 191, 22)\n",
      "  Min: -20.082091, Max: 16.627052\n",
      "  Mean: 0.000000, Std: 1.312890\n",
      "\n",
      "High-frequency component shape: (331, 191, 22)\n",
      "  Min: -6.299598, Max: 8.825853\n",
      "  Mean: 0.000000, Std: 0.355082\n"
     ]
    }
   ],
   "source": [
    "# Apply DWT to each stock-factor time series\n",
    "print(f\"Applying {WAVELET_TYPE} wavelet decomposition (level={WAVELET_LEVEL})...\")\n",
    "print(\"Processing each stock-factor combination...\\n\")\n",
    "\n",
    "# Initialize arrays for low and high frequency components\n",
    "# After decomposition, length will be approximately half\n",
    "approx_length = pywt.dwt_coeff_len(num_dates, pywt.Wavelet(WAVELET_TYPE).dec_len, mode='smooth')\n",
    "\n",
    "flow_low_freq = np.zeros((approx_length, num_stocks, num_factors))\n",
    "flow_high_freq = np.zeros((approx_length, num_stocks, num_factors))\n",
    "\n",
    "# Apply DWT to each time series\n",
    "for stock_idx in range(num_stocks):\n",
    "    if (stock_idx + 1) % 50 == 0:\n",
    "        print(f\"  Processing stock {stock_idx + 1}/{num_stocks}...\")\n",
    "    \n",
    "    for factor_idx in range(num_factors):\n",
    "        # Extract time series for this stock-factor combination\n",
    "        time_series = factor_tensor[:, stock_idx, factor_idx]\n",
    "        \n",
    "        # Handle NaN/Inf values\n",
    "        if np.isnan(time_series).any() or np.isinf(time_series).any():\n",
    "            time_series = np.nan_to_num(time_series, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        # Apply DWT\n",
    "        cA, cD = pywt.dwt(time_series, WAVELET_TYPE, mode='smooth')\n",
    "        \n",
    "        # Store coefficients\n",
    "        flow_low_freq[:, stock_idx, factor_idx] = cA  # Approximation (low freq)\n",
    "        flow_high_freq[:, stock_idx, factor_idx] = cD  # Detail (high freq)\n",
    "\n",
    "print(f\"\\nWavelet decomposition complete!\")\n",
    "print(f\"\\nLow-frequency component shape: {flow_low_freq.shape}\")\n",
    "print(f\"  Min: {flow_low_freq.min():.6f}, Max: {flow_low_freq.max():.6f}\")\n",
    "print(f\"  Mean: {flow_low_freq.mean():.6f}, Std: {flow_low_freq.std():.6f}\")\n",
    "\n",
    "print(f\"\\nHigh-frequency component shape: {flow_high_freq.shape}\")\n",
    "print(f\"  Min: {flow_high_freq.min():.6f}, Max: {flow_high_freq.max():.6f}\")\n",
    "print(f\"  Mean: {flow_high_freq.mean():.6f}, Std: {flow_high_freq.std():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1632930a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved flow.npz to: /home/ubuntu/rajnish/Multitask-Stockformer/data/NIFTY200/Stock_NIFTY_2022-01-01_2024-08-31/flow.npz\n",
      "File size: 18.95 MB\n",
      "\n",
      "Verification:\n",
      "  Keys: ['low_freq', 'high_freq']\n",
      "  low_freq shape: (331, 191, 22)\n",
      "  high_freq shape: (331, 191, 22)\n"
     ]
    }
   ],
   "source": [
    "# Save flow.npz\n",
    "flow_output_path = OUTPUT_DIR / 'flow.npz'\n",
    "\n",
    "np.savez_compressed(\n",
    "    flow_output_path,\n",
    "    low_freq=flow_low_freq,\n",
    "    high_freq=flow_high_freq\n",
    ")\n",
    "\n",
    "print(f\"Saved flow.npz to: {flow_output_path}\")\n",
    "print(f\"File size: {flow_output_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Verify\n",
    "loaded = np.load(flow_output_path)\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"  Keys: {list(loaded.keys())}\")\n",
    "print(f\"  low_freq shape: {loaded['low_freq'].shape}\")\n",
    "print(f\"  high_freq shape: {loaded['high_freq'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58496da8",
   "metadata": {},
   "source": [
    "## 5. Generate Trend Indicators\n",
    "\n",
    "Binary classification labels: 1 if return > 0 (up), else 0 (down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ecfda77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labels from: /home/ubuntu/rajnish/Multitask-Stockformer/data/NIFTY200/Stock_NIFTY_2022-01-01_2024-08-31/label.csv\n",
      "\n",
      "Label DataFrame shape: (95, 191)\n",
      "Date range: 2024-04-15 to 2024-08-30\n",
      "Stocks: 191\n",
      "\n",
      "Trend indicator shape: (95, 191)\n",
      "  [time=95, stock=191]\n",
      "\n",
      "Trend statistics:\n",
      "  Up days (1): 9555 (52.66%)\n",
      "  Down days (0): 8590 (47.34%)\n"
     ]
    }
   ],
   "source": [
    "# Load label.csv (daily returns)\n",
    "print(f\"Loading labels from: {LABEL_FILE}\")\n",
    "label_df = pd.read_csv(LABEL_FILE, index_col=0)\n",
    "\n",
    "print(f\"\\nLabel DataFrame shape: {label_df.shape}\")\n",
    "print(f\"Date range: {label_df.index[0]} to {label_df.index[-1]}\")\n",
    "print(f\"Stocks: {len(label_df.columns)}\")\n",
    "\n",
    "# Create binary trend indicators\n",
    "trend_indicator = (label_df.values > 0).astype(np.int32)\n",
    "\n",
    "print(f\"\\nTrend indicator shape: {trend_indicator.shape}\")\n",
    "print(f\"  [time={trend_indicator.shape[0]}, stock={trend_indicator.shape[1]}]\")\n",
    "print(f\"\\nTrend statistics:\")\n",
    "print(f\"  Up days (1): {(trend_indicator == 1).sum()} ({(trend_indicator == 1).mean()*100:.2f}%)\")\n",
    "print(f\"  Down days (0): {(trend_indicator == 0).sum()} ({(trend_indicator == 0).mean()*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2733f5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trend_indicator.npz to: /home/ubuntu/rajnish/Multitask-Stockformer/data/NIFTY200/Stock_NIFTY_2022-01-01_2024-08-31/trend_indicator.npz\n",
      "File size: 4.76 KB\n",
      "\n",
      "Verification:\n",
      "  Keys: ['trend']\n",
      "  trend shape: (95, 191)\n"
     ]
    }
   ],
   "source": [
    "# Save trend_indicator.npz\n",
    "trend_output_path = OUTPUT_DIR / 'trend_indicator.npz'\n",
    "\n",
    "np.savez_compressed(\n",
    "    trend_output_path,\n",
    "    trend=trend_indicator\n",
    ")\n",
    "\n",
    "print(f\"Saved trend_indicator.npz to: {trend_output_path}\")\n",
    "print(f\"File size: {trend_output_path.stat().st_size / 1024:.2f} KB\")\n",
    "\n",
    "# Verify\n",
    "loaded_trend = np.load(trend_output_path)\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"  Keys: {list(loaded_trend.keys())}\")\n",
    "print(f\"  trend shape: {loaded_trend['trend'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525f7eb4",
   "metadata": {},
   "source": [
    "## 6. Build Correlation Adjacency Matrix\n",
    "\n",
    "Calculate stock-stock correlation using factor values as features.\n",
    "Apply threshold to create sparse adjacency matrix for Graph Attention Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad09ad57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating stock-stock correlation matrix...\n",
      "Using 22 factors as features\n",
      "\n",
      "Stock features shape: (191, 22)\n",
      "  [stock=191, factor=22]\n",
      "\n",
      "Correlation matrix shape: (191, 191)\n",
      "Correlation statistics:\n",
      "  Min: -1.0000\n",
      "  Max: 1.0000\n",
      "  Mean: 0.0057\n",
      "  Median: 0.0027\n"
     ]
    }
   ],
   "source": [
    "# Calculate correlation matrix\n",
    "# Use average factor values over time as features for each stock\n",
    "print(\"Calculating stock-stock correlation matrix...\")\n",
    "print(f\"Using {num_factors} factors as features\\n\")\n",
    "\n",
    "# Average factor values across time for each stock: [stock, factor]\n",
    "stock_features = np.nanmean(factor_tensor, axis=0)  # [num_stocks, num_factors]\n",
    "\n",
    "print(f\"Stock features shape: {stock_features.shape}\")\n",
    "print(f\"  [stock={stock_features.shape[0]}, factor={stock_features.shape[1]}]\")\n",
    "\n",
    "# Calculate correlation matrix between all stock pairs\n",
    "corr_matrix = np.corrcoef(stock_features)\n",
    "\n",
    "print(f\"\\nCorrelation matrix shape: {corr_matrix.shape}\")\n",
    "print(f\"Correlation statistics:\")\n",
    "print(f\"  Min: {corr_matrix.min():.4f}\")\n",
    "print(f\"  Max: {corr_matrix.max():.4f}\")\n",
    "print(f\"  Mean: {corr_matrix.mean():.4f}\")\n",
    "print(f\"  Median: {np.median(corr_matrix):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "002348e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applied threshold: |correlation| >= 0.3\n",
      "\n",
      "Adjacency matrix statistics:\n",
      "  Non-zero entries: 27735\n",
      "  Sparsity: 23.97%\n",
      "  Average edges per node: 145.21\n",
      "\n",
      "Non-zero correlation distribution:\n",
      "  Min: -1.0000\n",
      "  Max: 0.9959\n",
      "  Mean: 0.0004\n",
      "  Median: -0.3008\n"
     ]
    }
   ],
   "source": [
    "# Apply threshold to create sparse adjacency matrix\n",
    "# Set correlations < threshold to 0 (remove weak edges)\n",
    "corr_adj = corr_matrix.copy()\n",
    "corr_adj[np.abs(corr_adj) < CORR_THRESHOLD] = 0\n",
    "\n",
    "# Set diagonal to 1 (self-loops)\n",
    "np.fill_diagonal(corr_adj, 1.0)\n",
    "\n",
    "print(f\"\\nApplied threshold: |correlation| >= {CORR_THRESHOLD}\")\n",
    "print(f\"\\nAdjacency matrix statistics:\")\n",
    "print(f\"  Non-zero entries: {(corr_adj != 0).sum()}\")\n",
    "print(f\"  Sparsity: {(corr_adj == 0).sum() / corr_adj.size * 100:.2f}%\")\n",
    "print(f\"  Average edges per node: {(corr_adj != 0).sum() / num_stocks:.2f}\")\n",
    "\n",
    "# Check distribution of correlation strengths\n",
    "non_diag_corr = corr_adj[~np.eye(corr_adj.shape[0], dtype=bool)]\n",
    "non_zero_corr = non_diag_corr[non_diag_corr != 0]\n",
    "\n",
    "if len(non_zero_corr) > 0:\n",
    "    print(f\"\\nNon-zero correlation distribution:\")\n",
    "    print(f\"  Min: {non_zero_corr.min():.4f}\")\n",
    "    print(f\"  Max: {non_zero_corr.max():.4f}\")\n",
    "    print(f\"  Mean: {non_zero_corr.mean():.4f}\")\n",
    "    print(f\"  Median: {np.median(non_zero_corr):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfa44f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved corr_adj.npy to: /home/ubuntu/rajnish/Multitask-Stockformer/data/NIFTY200/Stock_NIFTY_2022-01-01_2024-08-31/corr_adj.npy\n",
      "File size: 285.13 KB\n",
      "\n",
      "Verification:\n",
      "  Shape: (191, 191)\n",
      "  Non-zero entries: 27735\n"
     ]
    }
   ],
   "source": [
    "# Save correlation adjacency matrix\n",
    "corr_adj_path = OUTPUT_DIR / 'corr_adj.npy'\n",
    "\n",
    "np.save(corr_adj_path, corr_adj)\n",
    "\n",
    "print(f\"Saved corr_adj.npy to: {corr_adj_path}\")\n",
    "print(f\"File size: {corr_adj_path.stat().st_size / 1024:.2f} KB\")\n",
    "\n",
    "# Verify\n",
    "loaded_corr = np.load(corr_adj_path)\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"  Shape: {loaded_corr.shape}\")\n",
    "print(f\"  Non-zero entries: {(loaded_corr != 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a0010f",
   "metadata": {},
   "source": [
    "## 7. Generate Graph Embeddings (MANDATORY)\n",
    "\n",
    "**Important:** Graph embeddings are MANDATORY for Stockformer model.\n",
    "- Model architecture uses `adjgat` as spatial positional encoding\n",
    "- Added directly to features in `spatialAttention` layer: `x_ = x + adjgat`\n",
    "- Required shape: [num_stocks, 128]\n",
    "\n",
    "We'll use node2vec to generate 128-dimensional embeddings from the correlation adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "992db504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating graph from correlation adjacency matrix...\n",
      "Adjacency matrix shape: (191, 191)\n",
      "Non-zero edges: 27735\n",
      "\n",
      "Graph created:\n",
      "  Nodes: 191\n",
      "  Edges: 13772\n",
      "  Average degree: 144.21\n",
      "  Graph is connected ✓\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Convert correlation adjacency matrix to NetworkX graph\n",
    "print(\"Creating graph from correlation adjacency matrix...\")\n",
    "print(f\"Adjacency matrix shape: {corr_adj.shape}\")\n",
    "print(f\"Non-zero edges: {(corr_adj != 0).sum()}\")\n",
    "\n",
    "# Create weighted graph from adjacency matrix\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes (stocks)\n",
    "G.add_nodes_from(range(num_stocks))\n",
    "\n",
    "# Add edges with correlation weights\n",
    "edges_added = 0\n",
    "for i in range(num_stocks):\n",
    "    for j in range(i + 1, num_stocks):  # Only upper triangle to avoid duplicates\n",
    "        weight = corr_adj[i, j]\n",
    "        if weight != 0:  # Only add non-zero edges\n",
    "            G.add_edge(i, j, weight=abs(weight))  # Use absolute value as weight\n",
    "            edges_added += 1\n",
    "\n",
    "print(f\"\\nGraph created:\")\n",
    "print(f\"  Nodes: {G.number_of_nodes()}\")\n",
    "print(f\"  Edges: {G.number_of_edges()}\")\n",
    "print(f\"  Average degree: {sum(dict(G.degree()).values()) / G.number_of_nodes():.2f}\")\n",
    "\n",
    "# Check connectivity\n",
    "if nx.is_connected(G):\n",
    "    print(f\"  Graph is connected ✓\")\n",
    "else:\n",
    "    num_components = nx.number_connected_components(G)\n",
    "    print(f\"  Graph has {num_components} connected components\")\n",
    "    largest_cc = max(nx.connected_components(G), key=len)\n",
    "    print(f\"  Largest component size: {len(largest_cc)} nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a441195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating node2vec embeddings...\n",
      "Parameters:\n",
      "  Dimensions: 128\n",
      "  Walk length: 80\n",
      "  Number of walks: 10\n",
      "  Workers: 4\n",
      "  p=1, q=1 (balanced BFS/DFS)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf093c23707427d8b23a6d27f09c7bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/191 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|██████████| 3/3 [00:00<00:00,  3.34it/s]\n",
      "Generating walks (CPU: 2): 100%|██████████| 3/3 [00:00<00:00,  3.34it/s]\n",
      "Generating walks (CPU: 3): 100%|██████████| 2/2 [00:00<00:00,  3.42it/s]\n",
      "Generating walks (CPU: 4): 100%|██████████| 2/2 [00:00<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Word2Vec model on walks...\n",
      "Embedding generation complete!\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Generate node2vec embeddings\n",
    "# Parameters matching original paper's struc2vec setup\n",
    "print(\"\\nGenerating node2vec embeddings...\")\n",
    "print(\"Parameters:\")\n",
    "print(\"  Dimensions: 128\")\n",
    "print(\"  Walk length: 80\")\n",
    "print(\"  Number of walks: 10\")\n",
    "print(\"  Workers: 4\")\n",
    "print(\"  p=1, q=1 (balanced BFS/DFS)\")\n",
    "\n",
    "# Initialize node2vec\n",
    "node2vec_model = Node2Vec(\n",
    "    G,\n",
    "    dimensions=128,        # Embedding dimension (matches original 128_corr_struc2vec_adjgat.npy)\n",
    "    walk_length=80,        # Length of random walk (original paper parameter)\n",
    "    num_walks=10,          # Number of walks per node (original paper parameter)\n",
    "    workers=4,             # Parallel workers\n",
    "    p=1,                   # Return parameter (1 = balanced)\n",
    "    q=1,                   # In-out parameter (1 = balanced)\n",
    "    weight_key='weight',   # Use correlation weights\n",
    "    quiet=False\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Word2Vec model on walks...\")\n",
    "# Fit the model (this generates random walks and trains embeddings)\n",
    "model = node2vec_model.fit(\n",
    "    window=10,             # Context size\n",
    "    min_count=1,           # Minimum word count\n",
    "    batch_words=4,         # Batch size\n",
    "    epochs=20,             # Training epochs\n",
    "    sg=1,                  # Skip-gram (1) vs CBOW (0)\n",
    "    workers=4\n",
    ")\n",
    "\n",
    "print(\"Embedding generation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a5e26c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting embeddings for all stocks...\n",
      "\n",
      "Embedding matrix shape: (191, 128)\n",
      "  [stocks=191, dimensions=128]\n",
      "\n",
      "Embedding statistics:\n",
      "  Min: -0.546400\n",
      "  Max: 0.672861\n",
      "  Mean: -0.001186\n",
      "  Std: 0.102895\n",
      "  Zero vectors: 0\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Extract embeddings for all nodes\n",
    "print(\"Extracting embeddings for all stocks...\")\n",
    "\n",
    "# Create embedding matrix [num_stocks, 128]\n",
    "adjgat = np.zeros((num_stocks, 128))\n",
    "\n",
    "for node_id in range(num_stocks):\n",
    "    try:\n",
    "        adjgat[node_id] = model.wv[str(node_id)]\n",
    "    except KeyError:\n",
    "        # If node not in vocabulary (isolated node), use zero vector\n",
    "        print(f\"Warning: Node {node_id} not in vocabulary, using zero vector\")\n",
    "        adjgat[node_id] = np.zeros(128)\n",
    "\n",
    "print(f\"\\nEmbedding matrix shape: {adjgat.shape}\")\n",
    "print(f\"  [stocks={adjgat.shape[0]}, dimensions={adjgat.shape[1]}]\")\n",
    "print(f\"\\nEmbedding statistics:\")\n",
    "print(f\"  Min: {adjgat.min():.6f}\")\n",
    "print(f\"  Max: {adjgat.max():.6f}\")\n",
    "print(f\"  Mean: {adjgat.mean():.6f}\")\n",
    "print(f\"  Std: {adjgat.std():.6f}\")\n",
    "print(f\"  Zero vectors: {(np.abs(adjgat).sum(axis=1) == 0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d9012cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 128_corr_struc2vec_adjgat.npy to: /home/ubuntu/rajnish/Multitask-Stockformer/data/NIFTY200/Stock_NIFTY_2022-01-01_2024-08-31/128_corr_struc2vec_adjgat.npy\n",
      "File size: 191.12 KB\n",
      "\n",
      "Verification:\n",
      "  Shape: (191, 128)\n",
      "  Expected: (191, 128)\n",
      "  Match: True\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Save graph embeddings\n",
    "adjgat_path = OUTPUT_DIR / '128_corr_struc2vec_adjgat.npy'\n",
    "\n",
    "np.save(adjgat_path, adjgat)\n",
    "\n",
    "print(f\"Saved 128_corr_struc2vec_adjgat.npy to: {adjgat_path}\")\n",
    "print(f\"File size: {adjgat_path.stat().st_size / 1024:.2f} KB\")\n",
    "\n",
    "# Verify\n",
    "loaded_adjgat = np.load(adjgat_path)\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"  Shape: {loaded_adjgat.shape}\")\n",
    "print(f\"  Expected: ({num_stocks}, 128)\")\n",
    "print(f\"  Match: {loaded_adjgat.shape == (num_stocks, 128)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcb6405",
   "metadata": {},
   "source": [
    "## 8. Summary & Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "884dc70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 4 PREPROCESSING COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Output directory: /home/ubuntu/rajnish/Multitask-Stockformer/data/NIFTY200/Stock_NIFTY_2022-01-01_2024-08-31\n",
      "\n",
      "Generated files:\n",
      "  ✓ flow.npz                            - Wavelet-decomposed factors (low + high freq) (18.95 MB)\n",
      "  ✓ trend_indicator.npz                 - Binary trend labels (up/down) (4.76 KB)\n",
      "  ✓ corr_adj.npy                        - Stock correlation adjacency matrix (285.13 KB)\n",
      "  ✓ 128_corr_struc2vec_adjgat.npy       - Graph embeddings (node2vec, 128-dim) (191.12 KB)\n",
      "  ✓ label.csv                           - Daily returns (from Phase 3) (380.53 KB)\n",
      "\n",
      "Data dimensions:\n",
      "  Time steps (wavelet): 331\n",
      "  Time steps (original): 660\n",
      "  Stocks: 191\n",
      "  Factors: 22\n",
      "  Graph embedding dims: 128\n",
      "\n",
      "================================================================================\n",
      "ALL REQUIRED FILES GENERATED!\n",
      "================================================================================\n",
      "\n",
      "READY FOR PHASE 5: Model Configuration\n",
      "\n",
      "Next steps:\n",
      "1. Create config/Multitask_NIFTY200_Alpha158.conf\n",
      "   - Copy from config/Multitask_Stock.conf as template\n",
      "   - Update file paths to NIFTY200 data directory\n",
      "   - Set train/val/test splits\n",
      "\n",
      "2. Modify dataset loader: lib/Multitask_Stockformer_utils.py\n",
      "   - Update factor count: 360 → 22\n",
      "   - Update stock count: 255 → 191\n",
      "   - Fix hardcoded Alpha_360 paths to use config\n",
      "\n",
      "3. Test data loading\n",
      "   - Verify all files load correctly\n",
      "   - Check tensor shapes match model expectations\n",
      "\n",
      "4. Proceed to training (Phase 6)\n"
     ]
    }
   ],
   "source": [
    "# Summary of generated files\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 4 PREPROCESSING COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nOutput directory: {OUTPUT_DIR}\")\n",
    "print(f\"\\nGenerated files:\")\n",
    "\n",
    "files_to_check = [\n",
    "    ('flow.npz', 'Wavelet-decomposed factors (low + high freq)'),\n",
    "    ('trend_indicator.npz', 'Binary trend labels (up/down)'),\n",
    "    ('corr_adj.npy', 'Stock correlation adjacency matrix'),\n",
    "    ('128_corr_struc2vec_adjgat.npy', 'Graph embeddings (node2vec, 128-dim)'),\n",
    "    ('label.csv', 'Daily returns (from Phase 3)'),\n",
    "]\n",
    "\n",
    "for filename, description in files_to_check:\n",
    "    filepath = OUTPUT_DIR / filename\n",
    "    if filepath.exists():\n",
    "        size_mb = filepath.stat().st_size / 1024 / 1024\n",
    "        size_str = f\"{size_mb:.2f} MB\" if size_mb > 1 else f\"{filepath.stat().st_size / 1024:.2f} KB\"\n",
    "        print(f\"  ✓ {filename:35s} - {description} ({size_str})\")\n",
    "    else:\n",
    "        print(f\"  ✗ {filename:35s} - MISSING\")\n",
    "\n",
    "print(f\"\\nData dimensions:\")\n",
    "print(f\"  Time steps (wavelet): {approx_length}\")\n",
    "print(f\"  Time steps (original): {num_dates}\")\n",
    "print(f\"  Stocks: {num_stocks}\")\n",
    "print(f\"  Factors: {num_factors}\")\n",
    "print(f\"  Graph embedding dims: 128\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"ALL REQUIRED FILES GENERATED!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nREADY FOR PHASE 5: Model Configuration\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Create config/Multitask_NIFTY200_Alpha158.conf\")\n",
    "print(\"   - Copy from config/Multitask_Stock.conf as template\")\n",
    "print(\"   - Update file paths to NIFTY200 data directory\")\n",
    "print(\"   - Set train/val/test splits\")\n",
    "print(\"\\n2. Modify dataset loader: lib/Multitask_Stockformer_utils.py\")\n",
    "print(\"   - Update factor count: 360 → 22\")\n",
    "print(\"   - Update stock count: 255 → 191\")\n",
    "print(\"   - Fix hardcoded Alpha_360 paths to use config\")\n",
    "print(\"\\n3. Test data loading\")\n",
    "print(\"   - Verify all files load correctly\")\n",
    "print(\"   - Check tensor shapes match model expectations\")\n",
    "print(\"\\n4. Proceed to training (Phase 6)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phi3-k8s-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
