{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c2812df",
   "metadata": {},
   "source": [
    "# NIFTY-200 Phase 3: Alpha158 Factor Engineering and Preprocessing\n",
    "\n",
    "This notebook generates Alpha158 factors for the Multitask-Stockformer pipeline using Qlib formula definitions.\n",
    "\n",
    "## Approach\n",
    "\n",
    "- **Alpha158 factors**: Implemented using exact formulas from Qlib source code (158 features: 9 KBAR + 4 PRICE + 145 ROLLING)\n",
    "- **Calendar generation**: Trading calendar created from raw OHLCV data (660 trading days: 2022-01-03 to 2024-08-30)\n",
    "- **Neutralization**: Size proxy (log(close × rolling 60d volume)) + NSE sector dummies via cross-sectional OLS\n",
    "- **IC filtering**: Information Coefficient (IC) threshold |IC| >= 0.02\n",
    "- **Labels**: Daily returns (close-to-close)\n",
    "\n",
    "## Key Results\n",
    "\n",
    "- **Total factors computed**: 158 (all Alpha158 features)\n",
    "- **Factors after IC filter**: 22 (|IC| >= 0.02)\n",
    "- **Data shape**: 123,725 rows (660 dates × 191 symbols)\n",
    "- **Top factors**: STD20 (IC=0.029), KLEN (IC=0.029), BETA60 (IC=0.028)\n",
    "\n",
    "## Implementation Notes\n",
    "\n",
    "This implementation differs from the original Chinese stock project:\n",
    "- **Original project**: Used all 360 Alpha360 factors WITHOUT any IC filtering\n",
    "- **Our adaptation**: Uses 158 Alpha158 factors WITH strict IC filtering (|IC| >= 0.02)\n",
    "- **Rationale**: Indian market has different characteristics; IC filtering ensures only predictive factors are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc1d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-03 09:02:05,115 - INFO - Notebook setup complete\n"
     ]
    }
   ],
   "source": [
    "# Setup and Imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0dba2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Info:\n",
      "Python: 3.10.18\n",
      "NumPy: 1.26.4\n",
      "Pandas: 2.3.3\n",
      "Qlib available: True\n",
      "Statsmodels available: True\n",
      "PyTorch: 2.6.0+cpu, CUDA: False\n"
     ]
    }
   ],
   "source": [
    "# Environment Validation\n",
    "import platform\n",
    "print(\"Environment Info:\")\n",
    "print(f\"  Python: {platform.python_version()}\")\n",
    "print(f\"  NumPy: {np.__version__}\")\n",
    "print(f\"  Pandas: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042c1013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded and directories verified.\n",
      "WORKDIR: /home/ubuntu/rajnish/Multitask-Stockformer\n",
      "raw_dir: /home/ubuntu/rajnish/Multitask-Stockformer/data/NIFTY200/raw\n",
      "instruments_file: /home/ubuntu/rajnish/Multitask-Stockformer/data/NIFTY200/instruments/nifty200.txt\n",
      "qlib_dir: /home/ubuntu/rajnish/Multitask-Stockformer/data/NIFTY200/qlib_data/\n",
      "alpha_out_dir: /home/ubuntu/rajnish/Multitask-Stockformer/data/NIFTY200/Alpha_158_2022-01-01_2024-08-31/\n",
      "dataset_dir: /home/ubuntu/rajnish/Multitask-Stockformer/data/NIFTY200/Stock_NIFTY_2022-01-01_2024-08-31/\n",
      "label_out: /home/ubuntu/rajnish/Multitask-Stockformer/data/NIFTY200/Stock_NIFTY_2022-01-01_2024-08-31/label.csv\n",
      "size_proxy_pivot: /home/ubuntu/rajnish/Multitask-Stockformer/data/NIFTY200/size_proxy_pivot.csv\n",
      "sector_dummies: /home/ubuntu/rajnish/Multitask-Stockformer/data/NIFTY200/stock_info_with_dummies.csv\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "WORKDIR = \"/home/ubuntu/rajnish/Multitask-Stockformer\"\n",
    "\n",
    "RAW_DIR = os.path.join(WORKDIR, 'data/NIFTY200/raw')\n",
    "INSTRUMENTS_FILE = os.path.join(WORKDIR, 'data/NIFTY200/instruments/nifty200.txt')\n",
    "OUTPUT_DIR = os.path.join(WORKDIR, 'data/NIFTY200/Alpha_158_2022-01-01_2024-08-31')\n",
    "DATASET_DIR = os.path.join(WORKDIR, 'data/NIFTY200/Stock_NIFTY_2022-01-01_2024-08-31')\n",
    "LABEL_FILE = os.path.join(DATASET_DIR, 'label.csv')\n",
    "SIZE_PROXY_FILE = os.path.join(WORKDIR, 'data/NIFTY200/size_proxy_pivot.csv')\n",
    "SECTOR_DUMMIES_FILE = os.path.join(WORKDIR, 'data/NIFTY200/stock_info_with_dummies.csv')\n",
    "CALENDAR_FILE = os.path.join(WORKDIR, 'data/NIFTY200/qlib_data/calendars/day.txt')\n",
    "\n",
    "IC_THRESHOLD = 0.02\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(DATASET_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  RAW_DIR: {RAW_DIR}\")\n",
    "print(f\"  OUTPUT_DIR: {OUTPUT_DIR}\")\n",
    "print(f\"  IC_THRESHOLD: {IC_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b9daf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning 191 CSV files for trading dates...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 660 unique trading dates\n",
      "Date range: 2022-01-03 to 2024-08-30\n",
      "✓ Calendar file created: /home/ubuntu/rajnish/Multitask-Stockformer/data/NIFTY200/qlib_data/calendars/day.txt\n",
      "  Total trading days: 660\n",
      "\n",
      "First 5 dates:\n",
      "  2022-01-03\n",
      "  2022-01-04\n",
      "  2022-01-05\n",
      "  2022-01-06\n",
      "  2022-01-07\n",
      "\n",
      "Last 5 dates:\n",
      "  2024-08-26\n",
      "  2024-08-27\n",
      "  2024-08-28\n",
      "  2024-08-29\n",
      "  2024-08-30\n"
     ]
    }
   ],
   "source": [
    "# Generate trading calendar from raw OHLCV data\n",
    "import glob\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Use absolute paths\n",
    "WORKDIR = \"/home/ubuntu/rajnish/Multitask-Stockformer\"\n",
    "RAW_DIR_ABS = os.path.join(WORKDIR, \"data/NIFTY200/raw\")\n",
    "\n",
    "# Collect all unique dates from CSVs\n",
    "all_dates = set()\n",
    "csv_files = glob.glob(os.path.join(RAW_DIR_ABS, '*.csv'))\n",
    "\n",
    "print(f\"Scanning {len(csv_files)} CSV files for trading dates...\")\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    all_dates.update(df['Date'].dt.date)\n",
    "\n",
    "# Sort dates\n",
    "sorted_dates = sorted(all_dates)\n",
    "print(f\"Found {len(sorted_dates)} unique trading dates\")\n",
    "if sorted_dates:\n",
    "    print(f\"Date range: {sorted_dates[0]} to {sorted_dates[-1]}\")\n",
    "\n",
    "# Create calendar directory and file\n",
    "calendar_dir = os.path.join(WORKDIR, \"data/NIFTY200/qlib_data/calendars\")\n",
    "os.makedirs(calendar_dir, exist_ok=True)\n",
    "calendar_file = os.path.join(calendar_dir, \"day.txt\")\n",
    "\n",
    "# Write calendar file\n",
    "with open(calendar_file, 'w') as f:\n",
    "    for date in sorted_dates:\n",
    "        f.write(date.strftime('%Y-%m-%d') + '\\n')\n",
    "\n",
    "print(f\"✓ Calendar file created: {calendar_file}\")\n",
    "print(f\"  Total trading days: {len(sorted_dates)}\")\n",
    "\n",
    "# Display first and last 5 dates\n",
    "if sorted_dates:\n",
    "    print(\"\\nFirst 5 dates:\")\n",
    "    for d in sorted_dates[:5]:\n",
    "        print(f\"  {d}\")\n",
    "    print(\"\\nLast 5 dates:\")\n",
    "    for d in sorted_dates[-5:]:\n",
    "        print(f\"  {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303288af",
   "metadata": {},
   "source": [
    "## Alternative Approach: Implement Alpha158 Formulas Directly\n",
    "\n",
    "Since Qlib's CSV provider needs specific binary format, let's implement the Alpha158 formulas directly using pandas. This gives us:\n",
    "1. Exact 158 features as defined in Qlib source\n",
    "2. Full transparency and control\n",
    "3. Guaranteed compatibility with our NIFTY-200 data\n",
    "\n",
    "The formulas are extracted from Qlib's `Alpha158DL.get_feature_config()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53068fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw OHLCV data...\n",
      "Loaded 5 symbols\n",
      "\n",
      "Computing Alpha158 factors (test run with 5 symbols)...\n",
      "✓ Computed 158 Alpha158 factors\n",
      "  Expected: 158 (9 KBAR + 4 PRICE + 145 ROLLING)\n",
      "  Data shape: (3300, 158)\n",
      "\n",
      "✓ Test successful!\n",
      "  Shape: (3300, 158)\n",
      "  Features: 158\n",
      "\n",
      "First 10 features:\n",
      "  1. KMID\n",
      "  2. KLEN\n",
      "  3. KMID2\n",
      "  4. KUP\n",
      "  5. KUP2\n",
      "  6. KLOW\n",
      "  7. KLOW2\n",
      "  8. KSFT\n",
      "  9. KSFT2\n",
      "  10. OPEN0\n"
     ]
    }
   ],
   "source": [
    "# Import our Alpha158 implementation\n",
    "import sys\n",
    "sys.path.insert(0, '/home/ubuntu/rajnish/Multitask-Stockformer/data_processing_script/nifty')\n",
    "from alpha158_pandas import compute_alpha158_factors, get_feature_names\n",
    "\n",
    "# Load raw OHLCV data for all symbols\n",
    "print(\"Loading raw OHLCV data...\")\n",
    "ohlcv_data = {}\n",
    "for sym in instruments[:5]:  # Test with first 5 symbols\n",
    "    csv_path = os.path.join(RAW_DIR_ABS, f\"{sym}.csv\")\n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        ohlcv_data[sym] = df\n",
    "\n",
    "print(f\"Loaded {len(ohlcv_data)} symbols\")\n",
    "\n",
    "# Compute Alpha158 factors (TEST with small sample first)\n",
    "print(\"\\nComputing Alpha158 factors (test run with 5 symbols)...\")\n",
    "alpha158_test = compute_alpha158_factors(ohlcv_data)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\n✓ Test successful!\")\n",
    "print(f\"  Shape: {alpha158_test.shape}\")\n",
    "print(f\"  Features: {alpha158_test.shape[1]}\")\n",
    "print(f\"\\nFirst 10 features:\")\n",
    "for i, col in enumerate(alpha158_test.columns[:10]):\n",
    "    print(f\"  {i+1}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df6fc34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw OHLCV data for all 191 symbols...\n",
      "Loaded 191 symbols\n",
      "\n",
      "Computing Alpha158 factors for all symbols...\n",
      "✓ Computed 158 Alpha158 factors\n",
      "  Expected: 158 (9 KBAR + 4 PRICE + 145 ROLLING)\n",
      "  Data shape: (123725, 158)\n",
      "\n",
      "✓ Alpha158 computation complete!\n",
      "  Time elapsed: 475.2s\n",
      "  Shape: (123725, 158)\n",
      "  Features: 158\n",
      "  Date range: 2022-01-03 00:00:00 to 2024-08-30 00:00:00\n",
      "\n",
      "Feature breakdown:\n",
      "  KBAR: 9 features\n",
      "  PRICE: 4 features\n",
      "  ROLLING: 145 features\n",
      "  TOTAL: 158 features ✓\n"
     ]
    }
   ],
   "source": [
    "# Compute Alpha158 for ALL 191 symbols\n",
    "print(\"Loading raw OHLCV data for all 191 symbols...\")\n",
    "ohlcv_data_full = {}\n",
    "for sym in instruments:\n",
    "    csv_path = os.path.join(RAW_DIR_ABS, f\"{sym}.csv\")\n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        ohlcv_data_full[sym] = df\n",
    "\n",
    "print(f\"Loaded {len(ohlcv_data_full)} symbols\")\n",
    "\n",
    "# Compute Alpha158 factors for all symbols\n",
    "print(\"\\nComputing Alpha158 factors for all symbols...\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "alpha158_full = compute_alpha158_factors(ohlcv_data_full)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n✓ Alpha158 computation complete!\")\n",
    "print(f\"  Time elapsed: {elapsed:.1f}s\")\n",
    "print(f\"  Shape: {alpha158_full.shape}\")\n",
    "print(f\"  Features: {alpha158_full.shape[1]}\")\n",
    "print(f\"  Date range: {alpha158_full.index.get_level_values(0).min()} to {alpha158_full.index.get_level_values(0).max()}\")\n",
    "print(f\"\\nFeature breakdown:\")\n",
    "print(f\"  KBAR: 9 features\")\n",
    "print(f\"  PRICE: 4 features\")\n",
    "print(f\"  ROLLING: 145 features\")\n",
    "print(f\"  TOTAL: {alpha158_full.shape[1]} features ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "721ad979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading size proxy and sector dummies...\n",
      "✓ Size proxy shape: (631, 191)\n",
      "✓ Sector dummies shape: (191, 8)\n",
      "\n",
      "Neutralizing 158 factors...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing feature 20/158...\n",
      "  Processing feature 40/158...\n",
      "  Processing feature 60/158...\n",
      "  Processing feature 80/158...\n",
      "  Processing feature 100/158...\n",
      "  Processing feature 120/158...\n",
      "  Processing feature 140/158...\n",
      "\n",
      "✓ Neutralization complete!\n",
      "  Time elapsed: 229.0s\n",
      "  Neutralized 158 factors\n"
     ]
    }
   ],
   "source": [
    "# Neutralize Alpha158 factors (size + sector)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "print(\"Loading size proxy and sector dummies...\")\n",
    "# Load with absolute paths\n",
    "size_proxy_path = os.path.join(WORKDIR, \"data/NIFTY200/size_proxy_pivot.csv\")\n",
    "sector_dummies_path = os.path.join(WORKDIR, \"data/NIFTY200/stock_info_with_dummies.csv\")\n",
    "\n",
    "size_proxy = pd.read_csv(size_proxy_path)\n",
    "size_proxy['Date'] = pd.to_datetime(size_proxy['Date'])\n",
    "size_proxy = size_proxy.set_index('Date').sort_index()\n",
    "\n",
    "sector_df = pd.read_csv(sector_dummies_path)\n",
    "sector_cols = [c for c in sector_df.columns if c.startswith('SECTOR_')]\n",
    "sector_dummies = sector_df[['symbol'] + sector_cols].set_index('symbol')\n",
    "\n",
    "print(f\"✓ Size proxy shape: {size_proxy.shape}\")\n",
    "print(f\"✓ Sector dummies shape: {sector_dummies.shape}\")\n",
    "\n",
    "# Convert Alpha158 to pivoted format (dates × symbols) for each feature\n",
    "print(f\"\\nNeutralizing {alpha158_full.shape[1]} factors...\")\n",
    "\n",
    "neutralized_factors = {}\n",
    "start_time = time.time()\n",
    "\n",
    "for i, feature_name in enumerate(alpha158_full.columns, 1):\n",
    "    if i % 20 == 0:\n",
    "        print(f\"  Processing feature {i}/{alpha158_full.shape[1]}...\")\n",
    "    \n",
    "    # Pivot: dates × symbols\n",
    "    factor_pivot = alpha158_full[feature_name].unstack(level=1)\n",
    "    \n",
    "    # Neutralize cross-sectionally per date\n",
    "    neutralized_rows = []\n",
    "    for date in factor_pivot.index:\n",
    "        factor_vec = factor_pivot.loc[date]\n",
    "        \n",
    "        # Get size proxy for this date\n",
    "        if date in size_proxy.index:\n",
    "            size_vec = size_proxy.loc[date]\n",
    "        else:\n",
    "            # Skip this date if no size proxy\n",
    "            neutralized_rows.append(pd.Series(index=factor_vec.index, dtype=float))\n",
    "            continue\n",
    "        \n",
    "        # Align to common symbols\n",
    "        syms = factor_vec.index.intersection(size_vec.index).intersection(sector_dummies.index)\n",
    "        if len(syms) < 10:\n",
    "            neutralized_rows.append(pd.Series(index=factor_vec.index, dtype=float))\n",
    "            continue\n",
    "        \n",
    "        # Build X: [size, sector_dummies]\n",
    "        X = pd.concat([\n",
    "            size_vec.loc[syms].to_frame('size'),\n",
    "            sector_dummies.loc[syms]\n",
    "        ], axis=1).fillna(0).values\n",
    "        \n",
    "        y = factor_vec.loc[syms].values\n",
    "        \n",
    "        # Filter NaNs\n",
    "        mask = ~np.isnan(y)\n",
    "        if mask.sum() < 5:\n",
    "            neutralized_rows.append(pd.Series(index=factor_vec.index, dtype=float))\n",
    "            continue\n",
    "        \n",
    "        X_fit = X[mask]\n",
    "        y_fit = y[mask]\n",
    "        \n",
    "        # OLS regression\n",
    "        try:\n",
    "            lr = LinearRegression(fit_intercept=True)\n",
    "            lr.fit(X_fit, y_fit)\n",
    "            y_pred = lr.predict(X_fit)\n",
    "            residuals = y_fit - y_pred\n",
    "            \n",
    "            # Place residuals back\n",
    "            row = pd.Series(index=factor_vec.index, dtype=float)\n",
    "            row.loc[syms[mask]] = residuals\n",
    "            neutralized_rows.append(row)\n",
    "        except Exception:\n",
    "            neutralized_rows.append(pd.Series(index=factor_vec.index, dtype=float))\n",
    "    \n",
    "    neutralized_factors[feature_name] = pd.DataFrame(neutralized_rows, index=factor_pivot.index)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n✓ Neutralization complete!\")\n",
    "print(f\"  Time elapsed: {elapsed:.1f}s\")\n",
    "print(f\"  Neutralized {len(neutralized_factors)} factors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "042fe47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing factors and computing IC...\n",
      "✓ Labels loaded: (95, 191)\n",
      "  Processing feature 20/158...\n",
      "  Processing feature 40/158...\n",
      "  Processing feature 60/158...\n",
      "  Processing feature 80/158...\n",
      "  Processing feature 100/158...\n",
      "  Processing feature 120/158...\n",
      "  Processing feature 140/158...\n",
      "\n",
      "✓ Standardization and IC filtering complete!\n",
      "  Total factors: 158\n",
      "  Passed IC filter (|IC| >= 0.02): 22\n",
      "\n",
      "Top 10 factors by |IC|:\n",
      "factor        IC  selected\n",
      " STD20  0.028815      True\n",
      "  KLEN  0.028744      True\n",
      "BETA60  0.028330      True\n",
      "BETA20  0.027048      True\n",
      " HIGH0  0.027027      True\n",
      " STD10  0.026844      True\n",
      " MIN60 -0.026762      True\n",
      "   KUP  0.026388      True\n",
      "RESI20 -0.026228      True\n",
      "QTLD60 -0.025968      True\n"
     ]
    }
   ],
   "source": [
    "# Standardize (Z-score per date) and compute IC\n",
    "print(\"Standardizing factors and computing IC...\")\n",
    "\n",
    "# Load labels\n",
    "label_path = os.path.join(WORKDIR, \"data/NIFTY200/Stock_NIFTY_2022-01-01_2024-08-31/label.csv\")\n",
    "label_df = pd.read_csv(label_path, index_col=0, parse_dates=True)\n",
    "label_df = label_df.sort_index()\n",
    "print(f\"✓ Labels loaded: {label_df.shape}\")\n",
    "\n",
    "# Standardize and compute IC for each factor\n",
    "standardized_factors = {}\n",
    "ic_summary = []\n",
    "ic_threshold = 0.02\n",
    "\n",
    "for i, (feature_name, factor_mat) in enumerate(neutralized_factors.items(), 1):\n",
    "    if i % 20 == 0:\n",
    "        print(f\"  Processing feature {i}/{len(neutralized_factors)}...\")\n",
    "    \n",
    "    # Z-score per date (cross-sectional)\n",
    "    z_rows = []\n",
    "    for date in factor_mat.index:\n",
    "        vals = factor_mat.loc[date].values.astype(float)\n",
    "        mask = np.isfinite(vals)\n",
    "        if mask.sum() < 5:\n",
    "            z_rows.append(pd.Series([np.nan] * len(vals), index=factor_mat.columns))\n",
    "            continue\n",
    "        m = vals[mask].mean()\n",
    "        s = vals[mask].std()\n",
    "        if s == 0 or not np.isfinite(s):\n",
    "            z_rows.append(pd.Series([np.nan] * len(vals), index=factor_mat.columns))\n",
    "        else:\n",
    "            z = (vals - m) / s\n",
    "            z_rows.append(pd.Series(z, index=factor_mat.columns))\n",
    "    \n",
    "    z_mat = pd.DataFrame(z_rows, index=factor_mat.index)\n",
    "    \n",
    "    # Compute IC (correlation with next-day returns)\n",
    "    # Align dates\n",
    "    common_dates = z_mat.index.intersection(label_df.index)\n",
    "    ic_vals = []\n",
    "    \n",
    "    for date in common_dates:\n",
    "        # Factor values at date\n",
    "        x = z_mat.loc[date].values.astype(float)\n",
    "        # Next-day returns\n",
    "        next_date_idx = label_df.index.get_loc(date) + 1\n",
    "        if next_date_idx >= len(label_df):\n",
    "            continue\n",
    "        next_date = label_df.index[next_date_idx]\n",
    "        y = label_df.loc[next_date].values.astype(float)\n",
    "        \n",
    "        # Align symbols\n",
    "        common_syms = z_mat.columns.intersection(label_df.columns)\n",
    "        if len(common_syms) < 5:\n",
    "            continue\n",
    "        \n",
    "        x_aligned = z_mat.loc[date, common_syms].values.astype(float)\n",
    "        y_aligned = label_df.loc[next_date, common_syms].values.astype(float)\n",
    "        \n",
    "        mask = np.isfinite(x_aligned) & np.isfinite(y_aligned)\n",
    "        if mask.sum() < 5:\n",
    "            continue\n",
    "        \n",
    "        # Pearson correlation\n",
    "        try:\n",
    "            ic = np.corrcoef(x_aligned[mask], y_aligned[mask])[0, 1]\n",
    "            if np.isfinite(ic):\n",
    "                ic_vals.append(ic)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Mean IC\n",
    "    mean_ic = np.mean(ic_vals) if len(ic_vals) > 0 else np.nan\n",
    "    \n",
    "    # Save if passes threshold\n",
    "    keep = not np.isnan(mean_ic) and abs(mean_ic) >= ic_threshold\n",
    "    ic_summary.append({\n",
    "        'factor': feature_name,\n",
    "        'IC': mean_ic,\n",
    "        'selected': keep\n",
    "    })\n",
    "    \n",
    "    if keep:\n",
    "        standardized_factors[feature_name] = z_mat\n",
    "\n",
    "print(f\"\\n✓ Standardization and IC filtering complete!\")\n",
    "print(f\"  Total factors: {len(neutralized_factors)}\")\n",
    "print(f\"  Passed IC filter (|IC| >= {ic_threshold}): {len(standardized_factors)}\")\n",
    "\n",
    "# Display IC summary\n",
    "ic_df = pd.DataFrame(ic_summary).sort_values('IC', ascending=False, key=abs)\n",
    "print(f\"\\nTop 10 factors by |IC|:\")\n",
    "print(ic_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b75e41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving factors to: /home/ubuntu/rajnish/Multitask-Stockformer/data/NIFTY200/Alpha_158_2022-01-01_2024-08-31\n",
      "\n",
      "✓ Saved 22 factor files\n",
      "✓ Saved IC summary: /home/ubuntu/rajnish/Multitask-Stockformer/data/NIFTY200/Alpha_158_2022-01-01_2024-08-31/ic_summary.csv\n",
      "✓ Saved selected factors list: /home/ubuntu/rajnish/Multitask-Stockformer/data/NIFTY200/Alpha_158_2022-01-01_2024-08-31/selected_factors.txt\n",
      "\n",
      "============================================================\n",
      "PHASE 3 COMPLETE: Alpha158 Factor Engineering for NIFTY-200\n",
      "============================================================\n",
      "✓ Generated exactly 158 Alpha158 factors using Qlib formulas\n",
      "✓ Applied neutralization (size proxy + sector dummies)\n",
      "✓ Standardized factors (Z-score per date)\n",
      "✓ Filtered by IC (|IC| >= 0.02): 22 factors passed\n",
      "\n",
      "Output directory: /home/ubuntu/rajnish/Multitask-Stockformer/data/NIFTY200/Alpha_158_2022-01-01_2024-08-31\n",
      "Label file: /home/ubuntu/rajnish/Multitask-Stockformer/data/NIFTY200/Stock_NIFTY_2022-01-01_2024-08-31/label.csv\n",
      "\n",
      "Selected factors (22):\n",
      "  - STD20: IC = 0.0288\n",
      "  - KLEN: IC = 0.0287\n",
      "  - BETA60: IC = 0.0283\n",
      "  - BETA20: IC = 0.0270\n",
      "  - HIGH0: IC = 0.0270\n",
      "  - STD10: IC = 0.0268\n",
      "  - MIN60: IC = -0.0268\n",
      "  - KUP: IC = 0.0264\n",
      "  - RESI20: IC = -0.0262\n",
      "  - QTLD60: IC = -0.0260\n",
      "  ... and 12 more\n"
     ]
    }
   ],
   "source": [
    "# Save results to CSV files\n",
    "print(f\"Saving results to: {OUTPUT_DIR}\")\n",
    "\n",
    "# Save standardized factors that passed IC filter\n",
    "for name, mat in standardized_factors.items():\n",
    "    out_path = os.path.join(OUTPUT_DIR, f\"{name}.csv\")\n",
    "    mat.to_csv(out_path)\n",
    "\n",
    "# Save IC summary\n",
    "ic_summary_path = os.path.join(OUTPUT_DIR, \"ic_summary.csv\")\n",
    "ic_df.to_csv(ic_summary_path, index=False)\n",
    "\n",
    "# Save list of selected factors\n",
    "selected_factors = ic_df[ic_df['selected']]['factor'].tolist()\n",
    "selected_path = os.path.join(OUTPUT_DIR, \"selected_factors.txt\")\n",
    "with open(selected_path, 'w') as f:\n",
    "    for name in selected_factors:\n",
    "        f.write(name + '\\n')\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"PHASE 3 COMPLETE: Alpha158 Factor Engineering for NIFTY-200\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\n✓ Generated exactly 158 Alpha158 factors using Qlib formulas\")\n",
    "print(f\"✓ Applied neutralization (size proxy + sector dummies)\")\n",
    "print(f\"✓ Standardized factors (Z-score per date)\")\n",
    "print(f\"✓ Filtered by IC (|IC| >= {IC_THRESHOLD}): {len(selected_factors)} factors passed\")\n",
    "print(f\"\\nOutput:\")\n",
    "print(f\"  Factor files: {OUTPUT_DIR}\")\n",
    "print(f\"  Label file: {LABEL_FILE}\")\n",
    "print(f\"  IC summary: {ic_summary_path}\")\n",
    "print(f\"  Selected factors: {selected_path}\")\n",
    "print(f\"\\nTop 10 selected factors:\")\n",
    "for i, name in enumerate(selected_factors[:10], 1):\n",
    "    ic_val = ic_df[ic_df['factor'] == name]['IC'].values[0]\n",
    "    print(f\"  {i:2d}. {name:10s} IC = {ic_val:7.4f}\")\n",
    "if len(selected_factors) > 10:\n",
    "    print(f\"  ... and {len(selected_factors) - 10} more\")\n",
    "    \n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"COMPARISON WITH ORIGINAL CHINESE STOCK PROJECT\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nOriginal project (Chinese A-shares):\")\n",
    "print(f\"  - Total factors: 360 (Alpha360)\")\n",
    "print(f\"  - IC filtering: None\")\n",
    "print(f\"  - Final factors used: ALL 360\")\n",
    "print(f\"  - Stock universe: 255 stocks (CSI 300)\")\n",
    "print(f\"\\nOur NIFTY-200 adaptation:\")\n",
    "print(f\"  - Total factors: 158 (Alpha158)\")\n",
    "print(f\"  - IC filtering: |IC| >= {IC_THRESHOLD}\")\n",
    "print(f\"  - Final factors used: {len(selected_factors)} (strict filtering)\")\n",
    "print(f\"  - Stock universe: 191 stocks (NIFTY-200)\")\n",
    "print(f\"\\nRationale: IC filtering ensures only predictive factors are used,\")\n",
    "print(f\"           adapting the model to Indian market characteristics.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phi3-k8s-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
