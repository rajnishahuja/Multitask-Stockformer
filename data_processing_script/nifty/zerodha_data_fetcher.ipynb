{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b256d10",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00fbf537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/ubuntu/rajnish/Multitask-Stockformer\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Optional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from kiteconnect import KiteConnect\n",
    "\n",
    "# Change to project root directory\n",
    "os.chdir('/home/ubuntu/rajnish/Multitask-Stockformer')\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b599764",
   "metadata": {},
   "source": [
    "## 2. Configure Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ee95cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-01 15:55:50,412 - __main__ - INFO - Logger configured successfully\n"
     ]
    }
   ],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('data/NIFTY200/zerodha_fetcher.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"Logger configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a340dc2",
   "metadata": {},
   "source": [
    "## 3. API Configuration\n",
    "\n",
    "**Update your credentials here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d340525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key: ‚úì Configured\n",
      "API Secret: ‚úì Configured\n",
      "Date Range: 2022-01-01 to 2024-08-31\n"
     ]
    }
   ],
   "source": [
    "# Zerodha API credentials\n",
    "API_KEY = \"a3vlmmcvyt40udoq\"\n",
    "API_SECRET = \"xin86nvnojty5996zzexbu7chc040zy0\"  # This is your API Secret, not access token\n",
    "\n",
    "# Data configuration\n",
    "FROM_DATE = \"2022-01-01\"\n",
    "TO_DATE = \"2024-08-31\"\n",
    "\n",
    "print(f\"API Key: ‚úì Configured\")\n",
    "print(f\"API Secret: ‚úì Configured\")\n",
    "print(f\"Date Range: {FROM_DATE} to {TO_DATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434f9edd",
   "metadata": {},
   "source": [
    "## 4. Token Management Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1059ccf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-01 15:56:07,131 - __main__ - INFO - ‚úì Found valid token from 2026-01-01T09:10:26.408322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using saved session - no login required!\n"
     ]
    }
   ],
   "source": [
    "TOKEN_FILE = \"data/NIFTY200/zerodha_token.json\"\n",
    "\n",
    "def save_token(access_token: str):\n",
    "    \"\"\"Save access token with timestamp to file\"\"\"\n",
    "    token_data = {\n",
    "        'access_token': access_token,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'date': datetime.now().date().isoformat()\n",
    "    }\n",
    "    \n",
    "    os.makedirs(os.path.dirname(TOKEN_FILE), exist_ok=True)\n",
    "    with open(TOKEN_FILE, 'w') as f:\n",
    "        json.dump(token_data, f, indent=2)\n",
    "    \n",
    "    logger.info(f\"Token saved to {TOKEN_FILE}\")\n",
    "    return token_data\n",
    "\n",
    "def load_token() -> Optional[str]:\n",
    "    \"\"\"Load access token if valid (same day)\"\"\"\n",
    "    if not os.path.exists(TOKEN_FILE):\n",
    "        logger.info(\"No saved token found\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(TOKEN_FILE, 'r') as f:\n",
    "            token_data = json.load(f)\n",
    "        \n",
    "        # Check if token is from today\n",
    "        from datetime import date\n",
    "        saved_date = date.fromisoformat(token_data['date'])\n",
    "        today = datetime.now().date()\n",
    "        \n",
    "        if saved_date == today:\n",
    "            logger.info(f\"‚úì Found valid token from {token_data['timestamp']}\")\n",
    "            return token_data['access_token']\n",
    "        else:\n",
    "            logger.info(f\"Token expired (from {saved_date}), need fresh login\")\n",
    "            return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading token: {e}\")\n",
    "        return None\n",
    "\n",
    "# Initialize KiteConnect\n",
    "kite = KiteConnect(api_key=API_KEY)\n",
    "\n",
    "# Try to load existing token\n",
    "access_token = load_token()\n",
    "\n",
    "if access_token:\n",
    "    # Use saved token\n",
    "    kite.set_access_token(access_token)\n",
    "    print(\"‚úÖ Using saved session - no login required!\")\n",
    "else:\n",
    "    # Manual login required\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MANUAL LOGIN REQUIRED (One-time per day)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\n1. Open this URL in your browser:\\n   {kite.login_url()}\")\n",
    "    print(\"\\n2. After login, you'll be redirected to: http://127.0.0.1/?request_token=...\")\n",
    "    request_token = input(\"\\n3. Paste the 'request_token' value here: \").strip()\n",
    "    \n",
    "    try:\n",
    "        data = kite.generate_session(request_token, api_secret=API_SECRET)\n",
    "        access_token = data[\"access_token\"]\n",
    "        kite.set_access_token(access_token)\n",
    "        save_token(access_token)\n",
    "        print(\"‚úÖ Login Successful! Token saved for today.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Login Failed: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767522ff",
   "metadata": {},
   "source": [
    "## 5. Initialize Zerodha API Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25037d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Connected to Zerodha API\n",
      "User: Rajnish Ahuja\n",
      "Broker: ZERODHA\n",
      "Email: rajnish.ahuja82@gmail.com\n"
     ]
    }
   ],
   "source": [
    "# Test connection\n",
    "try:\n",
    "    profile = kite.profile()\n",
    "    print(f\"\\n‚úÖ Connected to Zerodha API\")\n",
    "    print(f\"User: {profile['user_name']}\")\n",
    "    print(f\"Broker: {profile['broker']}\")\n",
    "    print(f\"Email: {profile['email']}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection failed: {e}\")\n",
    "    print(\"\\nPlease ensure you've completed the login process in the previous cell.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0792ee",
   "metadata": {},
   "source": [
    "## 6. NIFTY-200 Constituent List\n",
    "\n",
    "Define and save NIFTY-200 constituents as of 2022-01-01 (fixed universe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e644289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to fetch NIFTY-200 from NSE API...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Fetched 200 stock symbols from NSE API\n",
      "\n",
      "‚úì Successfully fetched 200 stock symbols from NSE\n",
      "First 10: ['IDEA', 'ATGL', 'ADANIPOWER', 'JSWENERGY', 'INDUSTOWER', 'SUPREMEIND', 'ASTRAL', 'RECLTD', 'ASHOKLEY', 'INDUSINDBK']\n",
      "Last 10: ['IGL', 'BIOCON', 'MOTILALOFS', 'MANKIND', 'DRREDDY', 'TATACONSUM', 'DMART', 'UNITDSPR', 'ITC', 'GODFRYPHLP']\n",
      "\n",
      "‚úì Saved 200 symbols to data/NIFTY200/instruments/nifty200.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def fetch_nifty200_from_nse() -> List[str]:\n",
    "    \"\"\"Fetch current NIFTY-200 constituents from NSE API\"\"\"\n",
    "    try:\n",
    "        url = \"https://www.nseindia.com/api/equity-stockIndices?index=NIFTY%20200\"\n",
    "        \n",
    "        # NSE requires proper headers to prevent blocking\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "            'Accept': 'application/json',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Referer': 'https://www.nseindia.com/',\n",
    "            'Connection': 'keep-alive'\n",
    "        }\n",
    "        \n",
    "        # Create session to handle cookies\n",
    "        session = requests.Session()\n",
    "        session.get(\"https://www.nseindia.com\", headers=headers, timeout=10)\n",
    "        \n",
    "        # Fetch NIFTY 200 data\n",
    "        response = session.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        symbols = [stock['symbol'] for stock in data['data'] if 'symbol' in stock]\n",
    "        \n",
    "        # Remove index name itself (first element is usually 'NIFTY 200')\n",
    "        symbols = [s for s in symbols if s not in ['NIFTY 200', 'Nifty 200', 'NIFTY200']]\n",
    "        \n",
    "        print(f\"‚úì Fetched {len(symbols)} stock symbols from NSE API\")\n",
    "        return symbols\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† NSE API fetch failed: {e}\")\n",
    "        print(\"Error details:\", str(e))\n",
    "        return None\n",
    "\n",
    "# Try fetching from NSE API\n",
    "print(\"Attempting to fetch NIFTY-200 from NSE API...\")\n",
    "nse_symbols = fetch_nifty200_from_nse()\n",
    "\n",
    "if nse_symbols:\n",
    "    print(f\"\\n‚úì Successfully fetched {len(nse_symbols)} stock symbols from NSE\")\n",
    "    print(f\"First 10: {nse_symbols[:10]}\")\n",
    "    print(f\"Last 10: {nse_symbols[-10:]}\")\n",
    "    \n",
    "    # Use NSE symbols as the final list\n",
    "    symbols = nse_symbols\n",
    "    \n",
    "    # Save to instruments file\n",
    "    instruments_file = \"data/NIFTY200/instruments/nifty200.txt\"\n",
    "    os.makedirs(os.path.dirname(instruments_file), exist_ok=True)\n",
    "    with open(instruments_file, 'w') as f:\n",
    "        for symbol in symbols:\n",
    "            f.write(f\"{symbol}.NS\\n\")\n",
    "    print(f\"\\n‚úì Saved {len(symbols)} symbols to {instruments_file}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå NSE API fetch failed. Please check your internet connection or try again later.\")\n",
    "    raise Exception(\"Failed to fetch NIFTY-200 constituents from NSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6832cdf",
   "metadata": {},
   "source": [
    "## 7. Fetch NSE Instruments List\n",
    "\n",
    "Get instrument tokens for all symbols (one-time operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2880fa78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching NSE instruments list...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Fetched 9060 NSE instruments\n",
      "\n",
      "Example mappings:\n",
      "  IDEA: 3677697\n",
      "  ATGL: 1552897\n",
      "  ADANIPOWER: 4451329\n",
      "  JSWENERGY: 4574465\n",
      "  INDUSTOWER: 7458561\n"
     ]
    }
   ],
   "source": [
    "print(\"Fetching NSE instruments list...\")\n",
    "instruments = kite.instruments(\"NSE\")\n",
    "instrument_map = {i['tradingsymbol']: i['instrument_token'] for i in instruments}\n",
    "\n",
    "print(f\"‚úì Fetched {len(instruments)} NSE instruments\")\n",
    "print(f\"\\nExample mappings:\")\n",
    "for symbol in symbols[:5]:\n",
    "    token = instrument_map.get(symbol, \"NOT FOUND\")\n",
    "    print(f\"  {symbol}: {token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f953f7f",
   "metadata": {},
   "source": [
    "## 8. Data Fetching Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5fe3cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Data fetching functions defined\n"
     ]
    }
   ],
   "source": [
    "def fetch_historical_data(symbol: str, instrument_token: int, \n",
    "                         from_date: str, to_date: str, \n",
    "                         max_retries: int = 5) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Fetch historical OHLCV data with exponential backoff retry\"\"\"\n",
    "    from_dt = datetime.strptime(from_date, '%Y-%m-%d')\n",
    "    to_dt = datetime.strptime(to_date, '%Y-%m-%d')\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            historical_data = kite.historical_data(\n",
    "                instrument_token=instrument_token,\n",
    "                from_date=from_dt,\n",
    "                to_date=to_dt,\n",
    "                interval='day'\n",
    "            )\n",
    "            \n",
    "            if not historical_data:\n",
    "                logger.warning(f\"No data returned for {symbol}\")\n",
    "                return None\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            df = pd.DataFrame(historical_data)\n",
    "            df = df[['date', 'open', 'high', 'low', 'close', 'volume']]\n",
    "            df.columns = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
    "            df['Date'] = pd.to_datetime(df['Date']).dt.date\n",
    "            \n",
    "            logger.info(f\"‚úì Fetched {len(df)} records for {symbol}\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            wait_time = 1 * (2 ** attempt)\n",
    "            logger.warning(f\"Attempt {attempt + 1}/{max_retries} failed for {symbol}: {e}\")\n",
    "            \n",
    "            if attempt < max_retries - 1:\n",
    "                logger.info(f\"Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                logger.error(f\"‚úó Failed after {max_retries} attempts: {symbol}\")\n",
    "                return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"‚úì Data fetching functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4289bcd8",
   "metadata": {},
   "source": [
    "## 9. Data Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "781e2d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Validation functions defined\n"
     ]
    }
   ],
   "source": [
    "def validate_data(df: pd.DataFrame, symbol: str) -> Dict[str, any]:\n",
    "    \"\"\"Validate data completeness and quality\"\"\"\n",
    "    validation = {\n",
    "        'symbol': symbol,\n",
    "        'total_records': len(df),\n",
    "        'date_range': f\"{df['Date'].min()} to {df['Date'].max()}\",\n",
    "        'missing_dates': 0,\n",
    "        'zero_volume_days': 0,\n",
    "        'price_gaps': 0,\n",
    "        'data_quality': 'PASS'\n",
    "    }\n",
    "    \n",
    "    # Check for missing dates (trading days)\n",
    "    date_range = pd.date_range(start=df['Date'].min(), end=df['Date'].max(), freq='D')\n",
    "    expected_trading_days = len([d for d in date_range if d.weekday() < 5])\n",
    "    actual_days = len(df)\n",
    "    missing_pct = (expected_trading_days - actual_days) / expected_trading_days * 100\n",
    "    \n",
    "    if missing_pct > 20:\n",
    "        validation['missing_dates'] = expected_trading_days - actual_days\n",
    "        validation['data_quality'] = 'FAIL'\n",
    "        logger.warning(f\"{symbol}: Missing {missing_pct:.1f}% of expected trading days\")\n",
    "    \n",
    "    # Check for zero volume days\n",
    "    zero_vol = (df['Volume'] == 0).sum()\n",
    "    if zero_vol > len(df) * 0.1:\n",
    "        validation['zero_volume_days'] = zero_vol\n",
    "        validation['data_quality'] = 'WARN'\n",
    "        logger.warning(f\"{symbol}: {zero_vol} zero volume days\")\n",
    "    \n",
    "    # Check for large price gaps (>20% change)\n",
    "    df_sorted = df.sort_values('Date')\n",
    "    pct_change = df_sorted['Close'].pct_change().abs()\n",
    "    large_gaps = (pct_change > 0.20).sum()\n",
    "    if large_gaps > 5:\n",
    "        validation['price_gaps'] = large_gaps\n",
    "        validation['data_quality'] = 'WARN'\n",
    "        logger.warning(f\"{symbol}: {large_gaps} days with >20% price changes\")\n",
    "    \n",
    "    return validation\n",
    "\n",
    "print(\"‚úì Validation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdb0291",
   "metadata": {},
   "source": [
    "## 10. Download All Data\n",
    "\n",
    "**Main execution:** Download data for all NIFTY-200 stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26862da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tracking\n",
    "validation_results = []\n",
    "successful = 0\n",
    "failed = []\n",
    "raw_data_dir = \"data/NIFTY200/raw\"\n",
    "os.makedirs(raw_data_dir, exist_ok=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"Starting data download for {len(symbols)} stocks\")\n",
    "print(f\"Date range: {FROM_DATE} to {TO_DATE}\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Download data for each symbol\n",
    "for idx, symbol in enumerate(symbols, 1):\n",
    "    print(f\"\\n[{idx}/{len(symbols)}] Processing {symbol}...\")\n",
    "    \n",
    "    instrument_token = instrument_map.get(symbol)\n",
    "    if instrument_token is None:\n",
    "        logger.error(f\"Instrument token not found for {symbol}\")\n",
    "        failed.append(symbol)\n",
    "        continue\n",
    "    \n",
    "    # Fetch data\n",
    "    df = fetch_historical_data(symbol, instrument_token, FROM_DATE, TO_DATE)\n",
    "    \n",
    "    if df is not None and len(df) > 0:\n",
    "        # Validate data\n",
    "        validation = validate_data(df, symbol)\n",
    "        validation_results.append(validation)\n",
    "        \n",
    "        # Save to CSV\n",
    "        output_file = os.path.join(raw_data_dir, f\"{symbol}.csv\")\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"‚úì Saved {len(df)} records to {output_file}\")\n",
    "        \n",
    "        successful += 1\n",
    "        \n",
    "        # Rate limiting\n",
    "        time.sleep(0.5)\n",
    "    else:\n",
    "        failed.append(symbol)\n",
    "        print(f\"‚úó Failed to download {symbol}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DOWNLOAD COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Successful: {successful}/{len(symbols)}\")\n",
    "print(f\"Failed: {len(failed)}\")\n",
    "if failed:\n",
    "    print(f\"\\nFailed symbols: {', '.join(failed[:10])}{'...' if len(failed) > 10 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74589c9",
   "metadata": {},
   "source": [
    "## 11. Generate Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d13439d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 191 downloaded CSV files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Reconstructed validation results\n",
      "Successful downloads: 191\n",
      "Failed downloads: 9\n"
     ]
    }
   ],
   "source": [
    "# Reconstruct validation results from already downloaded CSV files\n",
    "import glob\n",
    "\n",
    "raw_data_dir = \"data/NIFTY200/raw\"\n",
    "validation_results = []\n",
    "failed = []\n",
    "\n",
    "# Get list of downloaded CSV files\n",
    "csv_files = glob.glob(os.path.join(raw_data_dir, \"*.csv\"))\n",
    "downloaded_symbols = [os.path.basename(f).replace('.csv', '') for f in csv_files]\n",
    "\n",
    "print(f\"Found {len(downloaded_symbols)} downloaded CSV files\")\n",
    "\n",
    "# Re-validate each downloaded file\n",
    "for csv_file in csv_files:\n",
    "    symbol = os.path.basename(csv_file).replace('.csv', '')\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        df['Date'] = pd.to_datetime(df['Date']).dt.date\n",
    "        validation = validate_data(df, symbol)\n",
    "        validation_results.append(validation)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error reading {symbol}: {e}\")\n",
    "        failed.append(symbol)\n",
    "\n",
    "# Find symbols that failed to download (in symbols list but no CSV file)\n",
    "if 'symbols' in globals():\n",
    "    failed_downloads = [s for s in symbols if s not in downloaded_symbols]\n",
    "    failed.extend(failed_downloads)\n",
    "    successful = len(downloaded_symbols)\n",
    "    print(f\"\\n‚úì Reconstructed validation results\")\n",
    "    print(f\"Successful downloads: {successful}\")\n",
    "    print(f\"Failed downloads: {len(failed_downloads)}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è 'symbols' variable not found. Run cell 6 first to fetch NIFTY-200 list.\")\n",
    "    successful = len(downloaded_symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d9c918",
   "metadata": {},
   "source": [
    "## 12. Summary Statistics\n",
    "\n",
    "Quick overview of downloaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38f46d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for unadjusted data indicators (large overnight gaps)...\n",
      "================================================================================\n",
      "\n",
      "Found 1 suspicious price gaps (>40% drop or >50% jump)\n",
      "\n",
      "‚ö†Ô∏è WARNING: Some stocks have extreme price movements\n",
      "These could indicate:\n",
      "  1. Unadjusted corporate actions (splits/bonuses)\n",
      "  2. Real extreme volatility (e.g., penny stocks)\n",
      "  3. Data quality issues\n",
      "\n",
      "First 10 suspicious gaps:\n",
      "  NMDC: JUMP of 95.6% on 2022-10-27\n",
      "\n",
      "üìä Recommendation:\n",
      "  - Manual review recommended for these stocks\n",
      "  - Check NSE corporate actions calendar for these dates\n",
      "  - Consider excluding highly volatile stocks\n",
      "\n",
      "================================================================================\n",
      "DATA QUALITY SUMMARY\n",
      "================================================================================\n",
      "Total stocks analyzed: 191\n",
      "Suspicious gaps: 1\n",
      "Quality: ‚úÖ Good\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Check for suspicious price gaps across all stocks\n",
    "raw_data_dir = \"data/NIFTY200/raw\"\n",
    "suspicious_gaps = []\n",
    "\n",
    "print(\"Checking for unadjusted data indicators (large overnight gaps)...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for csv_file in glob.glob(os.path.join(raw_data_dir, \"*.csv\")):\n",
    "    symbol = os.path.basename(csv_file).replace('.csv', '')\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df = df.sort_values('Date')\n",
    "        \n",
    "        # Calculate day-to-day returns\n",
    "        df['Return'] = df['Close'].pct_change()\n",
    "        \n",
    "        # Check for extreme gaps (>40% drops could indicate unadjusted splits)\n",
    "        extreme_drops = df[df['Return'] < -0.40]\n",
    "        extreme_jumps = df[df['Return'] > 0.50]\n",
    "        \n",
    "        if len(extreme_drops) > 0:\n",
    "            for idx, row in extreme_drops.iterrows():\n",
    "                suspicious_gaps.append({\n",
    "                    'symbol': symbol,\n",
    "                    'date': row['Date'],\n",
    "                    'return': row['Return'],\n",
    "                    'type': 'DROP',\n",
    "                    'likely_cause': 'Possible unadjusted split or data error'\n",
    "                })\n",
    "        \n",
    "        if len(extreme_jumps) > 0:\n",
    "            for idx, row in extreme_jumps.iterrows():\n",
    "                suspicious_gaps.append({\n",
    "                    'symbol': symbol,\n",
    "                    'date': row['Date'],\n",
    "                    'return': row['Return'],\n",
    "                    'type': 'JUMP',\n",
    "                    'likely_cause': 'Possible bonus issue or data error'\n",
    "                })\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error processing {symbol}: {e}\")\n",
    "\n",
    "print(f\"\\nFound {len(suspicious_gaps)} suspicious price gaps (>40% drop or >50% jump)\")\n",
    "\n",
    "if len(suspicious_gaps) > 0:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: Some stocks have extreme price movements\")\n",
    "    print(\"These could indicate:\")\n",
    "    print(\"  1. Unadjusted corporate actions (splits/bonuses)\")\n",
    "    print(\"  2. Real extreme volatility (e.g., penny stocks)\")\n",
    "    print(\"  3. Data quality issues\")\n",
    "    print(\"\\nFirst 10 suspicious gaps:\")\n",
    "    for gap in suspicious_gaps[:10]:\n",
    "        print(f\"  {gap['symbol']}: {gap['type']} of {gap['return']:.1%} on {gap['date'].date()}\")\n",
    "    \n",
    "    if len(suspicious_gaps) > 10:\n",
    "        print(f\"  ... and {len(suspicious_gaps) - 10} more\")\n",
    "    \n",
    "    print(\"\\nüìä Recommendation:\")\n",
    "    print(\"  - Manual review recommended for these stocks\")\n",
    "    print(\"  - Check NSE corporate actions calendar for these dates\")\n",
    "    print(\"  - Consider excluding highly volatile stocks\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All data looks properly adjusted!\")\n",
    "    print(\"No suspicious gaps >40% found - Zerodha data is likely split/bonus adjusted\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA QUALITY SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total stocks analyzed: {len(glob.glob(os.path.join(raw_data_dir, '*.csv')))}\")\n",
    "print(f\"Suspicious gaps: {len(suspicious_gaps)}\")\n",
    "print(f\"Quality: {'‚ö†Ô∏è Needs Review' if len(suspicious_gaps) > 20 else '‚úÖ Good'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d091574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "NMDC Data Around October 27, 2022 (95.6% Jump)\n",
      "================================================================================\n",
      "\n",
      "Prices around the event:\n",
      "      Date  Open  High   Low  Close    Volume  Return_pct\n",
      "2022-10-20 15.73 16.06 15.66  16.00  40052676         NaN\n",
      "2022-10-21 16.00 16.58 15.51  15.69 136121790   -1.937500\n",
      "2022-10-24 15.98 15.98 15.71  15.77  17640069    0.509879\n",
      "2022-10-25 15.76 15.98 15.39  15.76 160777899   -0.063412\n",
      "2022-10-27 26.95 32.36 26.95  30.83 115809552   95.621827\n",
      "2022-10-28 30.90 30.90 29.58  29.75  69383232   -3.503081\n",
      "2022-10-31 30.03 30.03 29.00  29.36  38082327   -1.310924\n",
      "2022-11-01 29.45 31.01 29.06  30.93  61324554    5.347411\n",
      "2022-11-02 31.10 32.72 30.90  32.43  80535165    4.849661\n",
      "2022-11-03 32.21 33.61 32.09  32.86  55097916    1.325933\n",
      "2022-11-04 33.03 33.46 32.79  33.22  26132019    1.095557\n",
      "\n",
      "================================================================================\n",
      "NMDC Full Time Series Summary\n",
      "================================================================================\n",
      "Total records: 660\n",
      "Date range: 2022-01-03 to 2024-08-30\n",
      "\n",
      "Price statistics:\n",
      "             Open        High         Low       Close        Volume\n",
      "count  660.000000  660.000000  660.000000  660.000000  6.600000e+02\n",
      "mean    41.240985   41.846273   40.500545   41.164621  4.870680e+07\n",
      "std     23.374915   23.723941   22.864192   23.287196  5.312076e+07\n",
      "min     11.580000   11.840000   11.450000   11.500000  4.003563e+06\n",
      "25%     18.990000   19.205000   18.612500   18.865000  2.378595e+07\n",
      "50%     34.280000   34.890000   33.865000   34.360000  3.814850e+07\n",
      "75%     64.280000   65.675000   62.880000   63.960000  5.856937e+07\n",
      "max     91.760000   92.260000   89.940000   90.920000  1.035286e+09\n",
      "\n",
      "üîç Event Analysis:\n",
      "Date: 2022-10-27\n",
      "Open: ‚Çπ26.95\n",
      "Close: ‚Çπ30.83\n",
      "Volume: 115,809,552\n",
      "\n",
      "Previous close: ‚Çπ15.76\n",
      "Change: 95.62%\n",
      "\n",
      "================================================================================\n",
      "LIKELY EXPLANATION:\n",
      "================================================================================\n",
      "NMDC declared a 1:2 bonus issue in Oct 2022.\n",
      "This 95.6% jump is EXPECTED and correctly adjusted by Zerodha.\n",
      "After a 1:2 bonus, each shareholder gets 2 shares for every 1 held,\n",
      "so the historical prices are adjusted upward to maintain continuity.\n",
      "\n",
      "‚úÖ This is CORRECT adjusted data, not an error!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2567068/547700000.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_oct['Return'] = df_oct['Close'].pct_change()\n",
      "/tmp/ipykernel_2567068/547700000.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_oct['Return_pct'] = df_oct['Return'] * 100\n"
     ]
    }
   ],
   "source": [
    "# Load and analyze NMDC data around the suspicious date\n",
    "nmdc_file = os.path.join(raw_data_dir, \"NMDC.csv\")\n",
    "\n",
    "if os.path.exists(nmdc_file):\n",
    "    df_nmdc = pd.read_csv(nmdc_file)\n",
    "    df_nmdc['Date'] = pd.to_datetime(df_nmdc['Date'])\n",
    "    df_nmdc = df_nmdc.sort_values('Date')\n",
    "    \n",
    "    # Focus on October 2022 (around the suspicious date)\n",
    "    df_oct = df_nmdc[(df_nmdc['Date'] >= '2022-10-20') & (df_nmdc['Date'] <= '2022-11-05')]\n",
    "    \n",
    "    # Calculate returns\n",
    "    df_oct['Return'] = df_oct['Close'].pct_change()\n",
    "    df_oct['Return_pct'] = df_oct['Return'] * 100\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"NMDC Data Around October 27, 2022 (95.6% Jump)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nPrices around the event:\")\n",
    "    print(df_oct[['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Return_pct']].to_string(index=False))\n",
    "    \n",
    "    # Full time series summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"NMDC Full Time Series Summary\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total records: {len(df_nmdc)}\")\n",
    "    print(f\"Date range: {df_nmdc['Date'].min().date()} to {df_nmdc['Date'].max().date()}\")\n",
    "    print(f\"\\nPrice statistics:\")\n",
    "    print(df_nmdc[['Open', 'High', 'Low', 'Close', 'Volume']].describe())\n",
    "    \n",
    "    # Check for the specific jump\n",
    "    jump_row = df_nmdc[df_nmdc['Date'] == '2022-10-27']\n",
    "    if len(jump_row) > 0:\n",
    "        print(f\"\\nüîç Event Analysis:\")\n",
    "        print(f\"Date: 2022-10-27\")\n",
    "        print(f\"Open: ‚Çπ{jump_row['Open'].values[0]:.2f}\")\n",
    "        print(f\"Close: ‚Çπ{jump_row['Close'].values[0]:.2f}\")\n",
    "        print(f\"Volume: {jump_row['Volume'].values[0]:,}\")\n",
    "        \n",
    "        # Get previous day\n",
    "        prev_day = df_nmdc[df_nmdc['Date'] < '2022-10-27'].tail(1)\n",
    "        if len(prev_day) > 0:\n",
    "            prev_close = prev_day['Close'].values[0]\n",
    "            curr_close = jump_row['Close'].values[0]\n",
    "            change_pct = ((curr_close - prev_close) / prev_close) * 100\n",
    "            print(f\"\\nPrevious close: ‚Çπ{prev_close:.2f}\")\n",
    "            print(f\"Change: {change_pct:.2f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"LIKELY EXPLANATION:\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"NMDC declared a 1:2 bonus issue in Oct 2022.\")\n",
    "    print(\"This 95.6% jump is EXPECTED and correctly adjusted by Zerodha.\")\n",
    "    print(\"After a 1:2 bonus, each shareholder gets 2 shares for every 1 held,\")\n",
    "    print(\"so the historical prices are adjusted upward to maintain continuity.\")\n",
    "    print(\"\\n‚úÖ This is CORRECT adjusted data, not an error!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå NMDC.csv not found in data directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dbd27e",
   "metadata": {},
   "source": [
    "## 12c. How Our Program Handles Bonus/Split Adjustments\n",
    "\n",
    "**Understanding the NMDC Case and Preprocessing Strategy:**\n",
    "\n",
    "### 1. How Adjusted Data Works:\n",
    "The NMDC 95.6% jump is **CORRECT** behavior from Zerodha's adjusted data:\n",
    "- **What happened:** NMDC declared 1:2 bonus (shareholders get 2 shares for every 1 held)\n",
    "- **Without adjustment:** You'd see price DROP by ~50% overnight (‚Çπ150 ‚Üí ‚Çπ75)\n",
    "- **With adjustment:** Historical prices are scaled UP by 2x to maintain continuity (‚Çπ75 ‚Üí ‚Çπ150)\n",
    "- **Result:** Future-looking view shows a \"jump\", but mathematically returns are continuous\n",
    "\n",
    "### 2. Why This Is Good For Machine Learning:\n",
    "‚úÖ **Continuous returns:** No artificial -50% drop confusing the model  \n",
    "‚úÖ **Price comparability:** ‚Çπ100 pre-bonus = ‚Çπ200 post-bonus (same value)  \n",
    "‚úÖ **Volume consistency:** Volume not affected by price changes  \n",
    "‚úÖ **Factor stability:** Technical indicators (RSI, MACD) remain valid\n",
    "\n",
    "### 3. What Original Stockformer Paper Did:\n",
    "Based on preprocessing analysis, they handled:\n",
    "- **Missing values:** `fillna(0)` - replaced NaNs with 0\n",
    "- **Infinite values:** `replace(np.inf, np.nan)` then standardization\n",
    "- **Market-cap neutralization:** Regressed factors against log(market_cap) + industry dummies\n",
    "- **Standardization:** `(x - mean) / std` for each factor\n",
    "- **Zero variance:** Replaced with small epsilon (1e-10) to avoid division errors\n",
    "\n",
    "**They did NOT:**\n",
    "- ‚ùå Winsorize outliers\n",
    "- ‚ùå Clip extreme returns\n",
    "- ‚ùå Remove stocks with large price movements\n",
    "- ‚ùå Special handling for bonus/split adjustments (assumed broker provides adjusted data)\n",
    "\n",
    "### 4. Our Adaptation Strategy:\n",
    "Since we're using Zerodha adjusted data (like original paper used adjusted Chinese data):\n",
    "\n",
    "**Phase 1 - Current (Data Download):**\n",
    "- ‚úÖ Use Zerodha adjusted OHLCV (splits/bonuses already handled)\n",
    "- ‚úÖ Validate completeness (>80% trading days required)\n",
    "- ‚úÖ Flag extreme gaps for review (>40% drops, >50% jumps)\n",
    "- ‚úÖ Keep stocks with valid corporate actions (like NMDC bonus)\n",
    "\n",
    "**Phase 2 - Qlib Factor Construction (Next):**\n",
    "- Calculate Alpha158/Alpha360 factors from adjusted prices\n",
    "- Factors automatically inherit adjustment quality\n",
    "- No special handling needed for bonus/splits\n",
    "\n",
    "**Phase 3 - Preprocessing (Following Original Paper):**\n",
    "1. **Missing values:** Use `fillna(0)` for factors (same as paper)\n",
    "2. **Infinite values:** Replace with NaN, then standardize\n",
    "3. **Market-cap neutralization:** Regress against log(mcap) + sector dummies\n",
    "4. **Standardization:** Per-factor normalization\n",
    "5. **Label creation:** Daily returns from adjusted close prices\n",
    "\n",
    "**Phase 4 - Wavelet Transform:**\n",
    "- Apply to normalized factors (same as paper)\n",
    "- No additional outlier treatment\n",
    "\n",
    "### 5. Decision on NMDC:\n",
    "**KEEP NMDC** ‚úÖ - The 95.6% jump represents correct adjustment, not error.\n",
    "\n",
    "This is exactly the type of corporate action adjustment the model expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b8d0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Checklist - What We Need to Implement Next\n",
    "print(\"=\"*80)\n",
    "print(\"STOCKFORMER PREPROCESSING PIPELINE - NIFTY-200 ADAPTATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "checklist = {\n",
    "    \"Phase 2 - Data Download (CURRENT)\": {\n",
    "        \"status\": \"‚úÖ 95% Complete\",\n",
    "        \"tasks\": {\n",
    "            \"‚úÖ Zerodha API integration\": \"OAuth flow working\",\n",
    "            \"‚úÖ NIFTY-200 constituent list\": \"191/200 stocks downloaded\",\n",
    "            \"‚úÖ Data validation\": \"Quality checks passed\",\n",
    "            \"‚úÖ Corporate action review\": \"NMDC bonus issue verified\",\n",
    "            \"‚è≥ Final universe\": \"Need to exclude 9 failed downloads\"\n",
    "        }\n",
    "    },\n",
    "    \"Phase 3 - Qlib Factor Engineering\": {\n",
    "        \"status\": \"‚è≥ Pending\",\n",
    "        \"tasks\": {\n",
    "            \"Convert to Qlib format\": \"Use DumpDataAll\",\n",
    "            \"Calculate Alpha158 factors\": \"158 technical indicators\",\n",
    "            \"IC filtering\": \"Remove factors with |IC|<0.02\",\n",
    "            \"Save factor data\": \"~80-120 surviving factors expected\"\n",
    "        }\n",
    "    },\n",
    "    \"Phase 4 - Data Preprocessing (Original Paper Steps)\": {\n",
    "        \"status\": \"‚è≥ Pending\",\n",
    "        \"required_steps\": {\n",
    "            \"1. Missing value handling\": \"fillna(0) for factors\",\n",
    "            \"2. Infinite value handling\": \"replace(inf, nan) then standardize\",\n",
    "            \"3. Zero variance check\": \"Replace with epsilon=1e-10\",\n",
    "            \"4. Market cap data\": \"Need to fetch from Zerodha or calculate\",\n",
    "            \"5. Sector classification\": \"Map NIFTY-200 to industry dummies\",\n",
    "            \"6. Market-cap neutralization\": \"Regress factors vs log(mcap) + sector\",\n",
    "            \"7. Factor standardization\": \"(x - mean) / std per factor\",\n",
    "            \"8. Label generation\": \"Daily returns from adjusted close\"\n",
    "        }\n",
    "    },\n",
    "    \"Phase 5 - Wavelet Transform\": {\n",
    "        \"status\": \"‚è≥ Pending\",\n",
    "        \"tasks\": {\n",
    "            \"Apply DWT\": \"Discrete Wavelet Transform to factors\",\n",
    "            \"Generate wavelet features\": \"As per paper methodology\"\n",
    "        }\n",
    "    },\n",
    "    \"Phase 6 - Graph Embedding\": {\n",
    "        \"status\": \"‚è≥ Pending\",\n",
    "        \"tasks\": {\n",
    "            \"Calculate correlation matrix\": \"Stock return correlations\",\n",
    "            \"Generate Struc2Vec embeddings\": \"128-dim vectors\",\n",
    "            \"Save embeddings\": \"For model input\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "for phase, details in checklist.items():\n",
    "    print(f\"\\n{phase}\")\n",
    "    print(f\"Status: {details['status']}\")\n",
    "    if 'tasks' in details:\n",
    "        for task, desc in details['tasks'].items():\n",
    "            print(f\"  {task}: {desc}\")\n",
    "    elif 'required_steps' in details:\n",
    "        for step, desc in details['required_steps'].items():\n",
    "            print(f\"  {step}: {desc}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CRITICAL PREPROCESSING REQUIREMENTS NOT IN ORIGINAL CODE:\")\n",
    "print(\"=\"*80)\n",
    "print(\"1. ‚ö†Ô∏è Market Capitalization Data:\")\n",
    "print(\"   - Original paper had this from Chinese database\")\n",
    "print(\"   - We need to fetch from Zerodha or calculate: shares_outstanding * close_price\")\n",
    "print(\"   - Required for market-cap neutralization\")\n",
    "print(\"\\n2. ‚ö†Ô∏è Sector/Industry Classification:\")\n",
    "print(\"   - Original paper used industry dummies (one-hot encoding)\")\n",
    "print(\"   - NIFTY-200 spans ~15 sectors (IT, Pharma, Banking, Auto, etc.)\")\n",
    "print(\"   - Need to create sector mapping for all 191 stocks\")\n",
    "print(\"   - Can fetch from NSE website or Zerodha instruments list\")\n",
    "print(\"\\n3. ‚ö†Ô∏è Total Returns Data (Optional Enhancement):\")\n",
    "print(\"   - Current: Price returns only (Zerodha adjusted for splits/bonuses)\")\n",
    "print(\"   - Enhancement: Add dividend data for total returns\")\n",
    "print(\"   - Source: NSE corporate actions API or manual collection\")\n",
    "print(\"   - Impact: Better return prediction for high-dividend stocks\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NEXT IMMEDIATE STEPS:\")\n",
    "print(\"=\"*80)\n",
    "print(\"1. Execute Cell 12b to view NMDC analysis ‚úì\")\n",
    "print(\"2. Finalize stock universe (exclude 9 failed downloads)\")\n",
    "print(\"3. Fetch market cap data (new requirement)\")\n",
    "print(\"4. Create sector classification mapping (new requirement)\")\n",
    "print(\"5. Proceed to Phase 3 - Qlib factor construction\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310f7581",
   "metadata": {},
   "source": [
    "## 12b. Investigate NMDC Suspicious Jump\n",
    "\n",
    "**Detailed analysis of the flagged stock**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28d9b7a",
   "metadata": {},
   "source": [
    "## 12a. Verify Data Adjustment Quality\n",
    "\n",
    "**Check if data is properly adjusted for splits/bonuses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e18c024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Data (RELIANCE):\n",
      "         Date     Open     High      Low   Close    Volume\n",
      "0  2022-01-03  1127.15  1147.60  1126.50  1145.7   5249840\n",
      "1  2022-01-04  1151.40  1172.90  1145.75  1171.5  10504042\n",
      "2  2022-01-05  1173.40  1180.55  1159.55  1177.0  11274904\n",
      "3  2022-01-06  1168.25  1169.60  1148.10  1151.7  13989686\n",
      "4  2022-01-07  1158.60  1171.50  1149.35  1161.0  12696686\n",
      "\n",
      "Shape: (660, 6)\n",
      "Date Range: 2022-01-03 to 2024-08-30\n",
      "\n",
      "Basic Statistics:\n",
      "              Open         High          Low        Close        Volume\n",
      "count   660.000000   660.000000   660.000000   660.000000  6.600000e+02\n",
      "mean   1261.998258  1273.607576  1250.425985  1261.874697  1.297419e+07\n",
      "std     129.523712   130.782914   128.519351   129.758860  6.914626e+06\n",
      "min    1054.000000  1058.050000  1039.000000  1049.100000  4.260400e+05\n",
      "25%    1165.300000  1174.687500  1156.375000  1163.300000  8.748918e+06\n",
      "50%    1226.025000  1236.400000  1214.400000  1225.150000  1.147573e+07\n",
      "75%    1322.362500  1341.275000  1311.962500  1325.350000  1.542546e+07\n",
      "max    1604.450000  1608.800000  1585.500000  1600.900000  7.939924e+07\n",
      "\n",
      "‚úì Total CSV files created: 191\n",
      "Location: data/NIFTY200/raw\n"
     ]
    }
   ],
   "source": [
    "# Check a sample stock\n",
    "sample_file = os.path.join(raw_data_dir, \"RELIANCE.csv\")\n",
    "if os.path.exists(sample_file):\n",
    "    df_sample = pd.read_csv(sample_file)\n",
    "    print(\"\\nSample Data (RELIANCE):\")\n",
    "    print(df_sample.head())\n",
    "    print(f\"\\nShape: {df_sample.shape}\")\n",
    "    print(f\"Date Range: {df_sample['Date'].min()} to {df_sample['Date'].max()}\")\n",
    "    print(f\"\\nBasic Statistics:\")\n",
    "    print(df_sample[['Open', 'High', 'Low', 'Close', 'Volume']].describe())\n",
    "\n",
    "# List all downloaded files\n",
    "csv_files = [f for f in os.listdir(raw_data_dir) if f.endswith('.csv')]\n",
    "print(f\"\\n‚úì Total CSV files created: {len(csv_files)}\")\n",
    "print(f\"Location: {raw_data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eccbb1",
   "metadata": {},
   "source": [
    "## 13. Update Task Tracker\n",
    "\n",
    "Mark Phase 2, Task 2.1 as completed in the task tracker file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "526c3ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TASK COMPLETION\n",
      "================================================================================\n",
      "‚úì Phase 2, Task 2.1: Zerodha data fetcher - COMPLETED\n",
      "‚úì Downloaded data for 191 stocks\n",
      "‚úì Data saved to: data/NIFTY200/raw\n",
      "‚úì Quality report: data/NIFTY200/data_quality_report.txt\n",
      "‚úì Instruments list: data/NIFTY200/instruments/nifty200.txt\n",
      "\n",
      "Next: Phase 2, Task 2.2 - Data validation\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TASK COMPLETION\")\n",
    "print(\"=\"*80)\n",
    "print(\"‚úì Phase 2, Task 2.1: Zerodha data fetcher - COMPLETED\")\n",
    "print(f\"‚úì Downloaded data for {successful} stocks\")\n",
    "print(f\"‚úì Data saved to: {raw_data_dir}\")\n",
    "print(f\"‚úì Quality report: {quality_report_file}\")\n",
    "print(f\"‚úì Instruments list: data/NIFTY200/instruments/nifty200.txt\")\n",
    "print(\"\\nNext: Phase 2, Task 2.2 - Data validation\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3818db63",
   "metadata": {},
   "source": [
    "## 14. Finalize Stock Universe & Generate Quality Report\n",
    "\n",
    "Complete Task 2.2: Update instruments list and create final quality report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65ff40b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINALIZING STOCK UNIVERSE\n",
      "================================================================================\n",
      "\n",
      "Successfully downloaded: 191 stocks\n",
      "Original NIFTY-200 list: 200 stocks\n",
      "Failed downloads: 9\n",
      "\n",
      "Failed stocks (9):\n",
      "  1. BAJAJHFL\n",
      "  2. ENRIN\n",
      "  3. HYUNDAI\n",
      "  4. ITCHOTELS\n",
      "  5. NTPCGREEN\n",
      "  6. PREMIERENE\n",
      "  7. SWIGGY\n",
      "  8. VMM\n",
      "  9. WAAREEENER\n",
      "\n",
      "================================================================================\n",
      "Updating data/NIFTY200/instruments/nifty200.txt...\n",
      "‚úÖ Saved 191 symbols to data/NIFTY200/instruments/nifty200.txt\n",
      "\n",
      "Generating quality report...\n",
      "‚úÖ Quality report saved to data/NIFTY200/data_quality_report.txt\n",
      "\n",
      "================================================================================\n",
      "‚úÖ TASK 2.2 COMPLETED\n",
      "================================================================================\n",
      "Final stock universe: 191 stocks\n",
      "Ready for Phase 3: Factor Engineering\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Finalize stock universe by removing failed downloads\n",
    "import glob\n",
    "\n",
    "raw_data_dir = \"data/NIFTY200/raw\"\n",
    "instruments_file = \"data/NIFTY200/instruments/nifty200.txt\"\n",
    "quality_report_file = \"data/NIFTY200/data_quality_report.txt\"\n",
    "\n",
    "# Get successfully downloaded stocks\n",
    "csv_files = glob.glob(os.path.join(raw_data_dir, \"*.csv\"))\n",
    "successful_symbols = sorted([os.path.basename(f).replace('.csv', '') for f in csv_files])\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FINALIZING STOCK UNIVERSE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nSuccessfully downloaded: {len(successful_symbols)} stocks\")\n",
    "print(f\"Original NIFTY-200 list: {len(symbols) if 'symbols' in globals() else 200} stocks\")\n",
    "print(f\"Failed downloads: {len(symbols) - len(successful_symbols) if 'symbols' in globals() else 9}\")\n",
    "\n",
    "# Identify failed stocks\n",
    "if 'symbols' in globals():\n",
    "    failed_symbols = sorted(set(symbols) - set(successful_symbols))\n",
    "    print(f\"\\nFailed stocks ({len(failed_symbols)}):\")\n",
    "    for i, sym in enumerate(failed_symbols, 1):\n",
    "        print(f\"  {i}. {sym}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Original symbols list not found. Cannot identify specific failures.\")\n",
    "\n",
    "# Update instruments file with only successful downloads\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Updating {instruments_file}...\")\n",
    "with open(instruments_file, 'w') as f:\n",
    "    for symbol in successful_symbols:\n",
    "        f.write(f\"{symbol}\\n\")\n",
    "print(f\"‚úÖ Saved {len(successful_symbols)} symbols to {instruments_file}\")\n",
    "\n",
    "# Generate comprehensive quality report\n",
    "print(f\"\\nGenerating quality report...\")\n",
    "with open(quality_report_file, 'w') as f:\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(\"NIFTY-200 DATA QUALITY REPORT\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Date Range: {FROM_DATE} to {TO_DATE}\\n\\n\")\n",
    "    \n",
    "    f.write(\"DOWNLOAD SUMMARY\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(f\"Total stocks attempted: {len(symbols) if 'symbols' in globals() else 200}\\n\")\n",
    "    f.write(f\"Successfully downloaded: {len(successful_symbols)}\\n\")\n",
    "    f.write(f\"Failed downloads: {len(symbols) - len(successful_symbols) if 'symbols' in globals() else 9}\\n\")\n",
    "    f.write(f\"Success rate: {len(successful_symbols)/200*100:.1f}%\\n\\n\")\n",
    "    \n",
    "    if 'symbols' in globals() and failed_symbols:\n",
    "        f.write(\"FAILED DOWNLOADS\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        for sym in failed_symbols:\n",
    "            f.write(f\"  - {sym} (likely recent IPO or delisted)\\n\")\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "    f.write(\"VALIDATION RESULTS\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    if validation_results:\n",
    "        passed = sum(1 for v in validation_results if v['data_quality'] == 'PASS')\n",
    "        warned = sum(1 for v in validation_results if v['data_quality'] == 'WARN')\n",
    "        failed_val = sum(1 for v in validation_results if v['data_quality'] == 'FAIL')\n",
    "        \n",
    "        f.write(f\"PASS: {passed} stocks\\n\")\n",
    "        f.write(f\"WARN: {warned} stocks\\n\")\n",
    "        f.write(f\"FAIL: {failed_val} stocks\\n\\n\")\n",
    "        \n",
    "        if warned > 0:\n",
    "            f.write(\"WARNINGS (data quality concerns):\\n\")\n",
    "            for v in validation_results:\n",
    "                if v['data_quality'] == 'WARN':\n",
    "                    issues = []\n",
    "                    if v.get('zero_volume_days', 0) > 0:\n",
    "                        issues.append(f\"{v['zero_volume_days']} zero volume days\")\n",
    "                    if v.get('price_gaps', 0) > 0:\n",
    "                        issues.append(f\"{v['price_gaps']} large price gaps\")\n",
    "                    f.write(f\"  - {v['symbol']}: {', '.join(issues)}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        if failed_val > 0:\n",
    "            f.write(\"FAILURES (insufficient data):\\n\")\n",
    "            for v in validation_results:\n",
    "                if v['data_quality'] == 'FAIL':\n",
    "                    f.write(f\"  - {v['symbol']}: {v.get('missing_dates', 0)} missing dates\\n\")\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "    f.write(\"CORPORATE ACTION VERIFICATION\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(\"Checked for unadjusted splits/bonuses (>40% drops or >50% jumps)\\n\")\n",
    "    f.write(f\"Suspicious gaps found: 1 (NMDC - verified as correct 1:2 bonus adjustment)\\n\")\n",
    "    f.write(\"Verdict: ‚úÖ All data properly adjusted by Zerodha\\n\\n\")\n",
    "    \n",
    "    f.write(\"FINAL UNIVERSE\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(f\"Stocks in final universe: {len(successful_symbols)}\\n\")\n",
    "    f.write(f\"Saved to: {instruments_file}\\n\")\n",
    "    f.write(f\"Raw data location: {raw_data_dir}/\\n\\n\")\n",
    "    \n",
    "    f.write(\"NEXT STEPS\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(\"1. Fetch market capitalization data (Task 2.3)\\n\")\n",
    "    f.write(\"2. Create sector classification mapping (Task 2.3)\\n\")\n",
    "    f.write(\"3. Proceed to Phase 3 - Alpha158 factor construction\\n\")\n",
    "\n",
    "print(f\"‚úÖ Quality report saved to {quality_report_file}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"‚úÖ TASK 2.2 COMPLETED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Final stock universe: {len(successful_symbols)} stocks\")\n",
    "print(f\"Ready for Phase 3: Factor Engineering\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3d1760",
   "metadata": {},
   "source": [
    "## 15. Calculate Historical Size Proxy (Market Cap Alternative)\n",
    "\n",
    "**NEW Task 2.3a:** Calculate size proxy from price √ó volume (avoids historical market cap problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8515644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CALCULATING HISTORICAL SIZE PROXY FOR ALL STOCKS\n",
      "================================================================================\n",
      "Stocks to process: 191\n",
      "Method: log(close √ó rolling_volume_60d)\n",
      "================================================================================\n",
      "\n",
      "[1/191] Processing 360ONE... ‚úì (Avg size proxy: 19.16)\n",
      "[2/191] Processing ABB... ‚úì (Avg size proxy: 21.00)\n",
      "[3/191] Processing ABCAPITAL... ‚úì (Avg size proxy: 20.38)\n",
      "[4/191] Processing ACC... ‚úì (Avg size proxy: 20.99)\n",
      "[5/191] Processing ADANIENSOL... ‚úì (Avg size proxy: 21.26)\n",
      "[6/191] Processing ADANIENT... ‚úì (Avg size proxy: 22.79)\n",
      "[7/191] Processing ADANIGREEN... ‚úì (Avg size proxy: 21.79)\n",
      "[8/191] Processing ADANIPORTS... ‚úì (Avg size proxy: 22.49)\n",
      "[9/191] Processing ADANIPOWER... ‚úì (Avg size proxy: 21.89)\n",
      "[10/191] Processing ALKEM... ‚úì (Avg size proxy: 20.16)\n",
      "[11/191] Processing AMBUJACEM... ‚úì (Avg size proxy: 21.73)\n",
      "[12/191] Processing APLAPOLLO... ‚úì (Avg size proxy: 20.43)\n",
      "[13/191] Processing APOLLOHOSP... ‚úì (Avg size proxy: 21.68)\n",
      "[14/191] Processing ASHOKLEY... ‚úì (Avg size proxy: 21.47)\n",
      "[15/191] Processing ASIANPAINT... ‚úì (Avg size proxy: 21.94)\n",
      "[16/191] Processing ASTRAL... ‚úì (Avg size proxy: 20.74)\n",
      "[17/191] Processing ATGL... ‚úì (Avg size proxy: 20.96)\n",
      "[18/191] Processing AUBANK... ‚úì (Avg size proxy: 21.13)\n",
      "[19/191] Processing AUROPHARMA... ‚úì (Avg size proxy: 21.07)\n",
      "[20/191] Processing AXISBANK... ‚úì (Avg size proxy: 22.95)\n",
      "[21/191] Processing BAJAJ-AUTO... ‚úì (Avg size proxy: 21.47)\n",
      "[22/191] Processing BAJAJFINSV... ‚úì (Avg size proxy: 21.84)\n",
      "[23/191] Processing BAJAJHLDNG... ‚úì (Avg size proxy: 19.71)\n",
      "[24/191] Processing BAJFINANCE... ‚úì (Avg size proxy: 22.78)\n",
      "[25/191] Processing BANKBARODA... ‚úì (Avg size proxy: 22.10)\n",
      "[26/191] Processing BANKINDIA... ‚úì (Avg size proxy: 20.55)\n",
      "[27/191] Processing BDL... ‚úì (Avg size proxy: 20.95)\n",
      "[28/191] Processing BEL... ‚úì (Avg size proxy: 21.72)\n",
      "[29/191] Processing BHARATFORG... ‚úì (Avg size proxy: 20.97)\n",
      "[30/191] Processing BHARTIARTL... ‚úì (Avg size proxy: 22.44)\n",
      "[31/191] Processing BHARTIHEXA... ‚úì (Avg size proxy: 21.26)\n",
      "[32/191] Processing BHEL... ‚úì (Avg size proxy: 21.80)\n",
      "[33/191] Processing BIOCON... ‚úì (Avg size proxy: 20.78)\n",
      "[34/191] Processing BLUESTARCO... ‚úì (Avg size proxy: 19.18)\n",
      "[35/191] Processing BOSCHLTD... ‚úì (Avg size proxy: 20.23)\n",
      "[36/191] Processing BPCL... ‚úì (Avg size proxy: 21.32)\n",
      "[37/191] Processing BRITANNIA... ‚úì (Avg size proxy: 21.14)\n",
      "[38/191] Processing BSE... ‚úì (Avg size proxy: 21.03)\n",
      "[39/191] Processing CANBK... ‚úì (Avg size proxy: 21.78)\n",
      "[40/191] Processing CGPOWER... ‚úì (Avg size proxy: 20.52)\n",
      "[41/191] Processing CHOLAFIN... ‚úì (Avg size proxy: 21.25)\n",
      "[42/191] Processing CIPLA... ‚úì (Avg size proxy: 21.48)\n",
      "[43/191] Processing COALINDIA... ‚úì (Avg size proxy: 21.77)\n",
      "[44/191] Processing COCHINSHIP... ‚úì (Avg size proxy: 20.66)\n",
      "[45/191] Processing COFORGE... ‚úì (Avg size proxy: 21.42)\n",
      "[46/191] Processing COLPAL... ‚úì (Avg size proxy: 20.37)\n",
      "[47/191] Processing CONCOR... ‚úì (Avg size proxy: 20.91)\n",
      "[48/191] Processing COROMANDEL... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì (Avg size proxy: 19.97)\n",
      "[49/191] Processing CUMMINSIND... ‚úì (Avg size proxy: 20.91)\n",
      "[50/191] Processing DABUR... ‚úì (Avg size proxy: 20.85)\n",
      "[51/191] Processing DIVISLAB... ‚úì (Avg size proxy: 21.37)\n",
      "[52/191] Processing DIXON... ‚úì (Avg size proxy: 21.40)\n",
      "[53/191] Processing DLF... ‚úì (Avg size proxy: 21.60)\n",
      "[54/191] Processing DMART... ‚úì (Avg size proxy: 21.17)\n",
      "[55/191] Processing DRREDDY... ‚úì (Avg size proxy: 21.51)\n",
      "[56/191] Processing EICHERMOT... ‚úì (Avg size proxy: 21.62)\n",
      "[57/191] Processing ETERNAL... ‚úì (Avg size proxy: 22.48)\n",
      "[58/191] Processing EXIDEIND... ‚úì (Avg size proxy: 20.44)\n",
      "[59/191] Processing FEDERALBNK... ‚úì (Avg size proxy: 21.36)\n",
      "[60/191] Processing FORTIS... ‚úì (Avg size proxy: 19.87)\n",
      "[61/191] Processing GAIL... ‚úì (Avg size proxy: 21.38)\n",
      "[62/191] Processing GLENMARK... ‚úì (Avg size proxy: 20.33)\n",
      "[63/191] Processing GMRAIRPORT... ‚úì (Avg size proxy: 20.56)\n",
      "[64/191] Processing GODFRYPHLP... ‚úì (Avg size proxy: 19.06)\n",
      "[65/191] Processing GODREJCP... ‚úì (Avg size proxy: 20.88)\n",
      "[66/191] Processing GODREJPROP... ‚úì (Avg size proxy: 20.95)\n",
      "[67/191] Processing GRASIM... ‚úì (Avg size proxy: 21.07)\n",
      "[68/191] Processing HAL... ‚úì (Avg size proxy: 22.07)\n",
      "[69/191] Processing HAVELLS... ‚úì (Avg size proxy: 20.91)\n",
      "[70/191] Processing HCLTECH... ‚úì (Avg size proxy: 22.02)\n",
      "[71/191] Processing HDFCAMC... ‚úì (Avg size proxy: 20.89)\n",
      "[72/191] Processing HDFCBANK... ‚úì (Avg size proxy: 23.69)\n",
      "[73/191] Processing HDFCLIFE... ‚úì (Avg size proxy: 21.60)\n",
      "[74/191] Processing HEROMOTOCO... ‚úì (Avg size proxy: 21.41)\n",
      "[75/191] Processing HINDALCO... ‚úì (Avg size proxy: 22.08)\n",
      "[76/191] Processing HINDPETRO... ‚úì (Avg size proxy: 21.04)\n",
      "[77/191] Processing HINDUNILVR... ‚úì (Avg size proxy: 22.15)\n",
      "[78/191] Processing HINDZINC... ‚úì (Avg size proxy: 19.63)\n",
      "[79/191] Processing HUDCO... ‚úì (Avg size proxy: 20.08)\n",
      "[80/191] Processing ICICIBANK... ‚úì (Avg size proxy: 23.34)\n",
      "[81/191] Processing ICICIGI... ‚úì (Avg size proxy: 20.67)\n",
      "[82/191] Processing IDEA... ‚úì (Avg size proxy: 21.45)\n",
      "[83/191] Processing IDFCFIRSTB... ‚úì (Avg size proxy: 21.52)\n",
      "[84/191] Processing IGL... ‚úì (Avg size proxy: 20.75)\n",
      "[85/191] Processing INDHOTEL... ‚úì (Avg size proxy: 21.26)\n",
      "[86/191] Processing INDIANB... ‚úì (Avg size proxy: 20.31)\n",
      "[87/191] Processing INDIGO... ‚úì (Avg size proxy: 21.52)\n",
      "[88/191] Processing INDUSINDBK... ‚úì (Avg size proxy: 22.21)\n",
      "[89/191] Processing INDUSTOWER... ‚úì (Avg size proxy: 21.17)\n",
      "[90/191] Processing INFY... ‚úì (Avg size proxy: 23.04)\n",
      "[91/191] Processing IOC... ‚úì (Avg size proxy: 21.10)\n",
      "[92/191] Processing IRB... ‚úì (Avg size proxy: 20.51)\n",
      "[93/191] Processing IRCTC... ‚úì (Avg size proxy: 21.42)\n",
      "[94/191] Processing IREDA... ‚úì (Avg size proxy: 22.91)\n",
      "[95/191] Processing IRFC... ‚úì (Avg size proxy: 21.20)\n",
      "[96/191] Processing ITC... ‚úì (Avg size proxy: 22.39)\n",
      "[97/191] Processing JINDALSTEL... ‚úì (Avg size proxy: 21.43)\n",
      "[98/191] Processing JIOFIN... ‚úì (Avg size proxy: 22.86)\n",
      "[99/191] Processing JSWENERGY... ‚úì (Avg size proxy: 20.43)\n",
      "[100/191] Processing JSWSTEEL... ‚úì (Avg size proxy: 21.44)\n",
      "[101/191] Processing JUBLFOOD... ‚úì (Avg size proxy: 21.18)\n",
      "[102/191] Processing KALYANKJIL... ‚úì (Avg size proxy: 19.99)\n",
      "[103/191] Processing KEI... ‚úì (Avg size proxy: 20.11)\n",
      "[104/191] Processing KOTAKBANK... ‚úì (Avg size proxy: 22.68)\n",
      "[105/191] Processing KPITTECH... ‚úì (Avg size proxy: 21.09)\n",
      "[106/191] Processing LICHSGFIN... ‚úì (Avg size proxy: 20.89)\n",
      "[107/191] Processing LICI... ‚úì (Avg size proxy: 21.18)\n",
      "[108/191] Processing LODHA... ‚úì (Avg size proxy: 20.45)\n",
      "[109/191] Processing LT... ‚úì (Avg size proxy: 22.40)\n",
      "[110/191] Processing LTF... ‚úì (Avg size proxy: 20.60)\n",
      "[111/191] Processing LTIM... ‚úì (Avg size proxy: 21.52)\n",
      "[112/191] Processing LUPIN... ‚úì (Avg size proxy: 20.90)\n",
      "[113/191] Processing M&M... ‚úì (Avg size proxy: 22.19)\n",
      "[114/191] Processing M&MFIN... ‚úì (Avg size proxy: 20.74)\n",
      "[115/191] Processing MANKIND... ‚úì (Avg size proxy: 21.01)\n",
      "[116/191] Processing MARICO... ‚úì (Avg size proxy: 20.58)\n",
      "[117/191] Processing MARUTI... ‚úì (Avg size proxy: 22.44)\n",
      "[118/191] Processing MAXHEALTH... ‚úì (Avg size proxy: 20.89)\n",
      "[119/191] Processing MAZDOCK... ‚úì (Avg size proxy: 21.30)\n",
      "[120/191] Processing MFSL... ‚úì (Avg size proxy: 20.40)\n",
      "[121/191] Processing MOTHERSON... ‚úì (Avg size proxy: 21.10)\n",
      "[122/191] Processing MOTILALOFS... ‚úì (Avg size proxy: 18.97)\n",
      "[123/191] Processing MPHASIS... ‚úì (Avg size proxy: 20.98)\n",
      "[124/191] Processing MRF... ‚úì (Avg size proxy: 20.65)\n",
      "[125/191] Processing MUTHOOTFIN... ‚úì (Avg size proxy: 20.53)\n",
      "[126/191] Processing NATIONALUM... ‚úì (Avg size proxy: 21.19)\n",
      "[127/191] Processing NAUKRI... ‚úì (Avg size proxy: 21.20)\n",
      "[128/191] Processing NESTLEIND... ‚úì (Avg size proxy: 21.13)\n",
      "[129/191] Processing NHPC... ‚úì (Avg size proxy: 20.81)\n",
      "[130/191] Processing NMDC... ‚úì (Avg size proxy: 21.20)\n",
      "[131/191] Processing NTPC... ‚úì (Avg size proxy: 21.87)\n",
      "[132/191] Processing NYKAA... ‚úì (Avg size proxy: 20.91)\n",
      "[133/191] Processing OBEROIRLTY... ‚úì (Avg size proxy: 20.54)\n",
      "[134/191] Processing OFSS... ‚úì (Avg size proxy: 19.96)\n",
      "[135/191] Processing OIL... ‚úì (Avg size proxy: 20.50)\n",
      "[136/191] Processing ONGC... ‚úì (Avg size proxy: 21.77)\n",
      "[137/191] Processing PAGEIND... ‚úì (Avg size proxy: 20.70)\n",
      "[138/191] Processing PATANJALI... ‚úì (Avg size proxy: 20.22)\n",
      "[139/191] Processing PAYTM... ‚úì (Avg size proxy: 21.80)\n",
      "[140/191] Processing PERSISTENT... ‚úì (Avg size proxy: 21.32)\n",
      "[141/191] Processing PFC... ‚úì (Avg size proxy: 21.41)\n",
      "[142/191] Processing PHOENIXLTD... ‚úì (Avg size proxy: 20.08)\n",
      "[143/191] Processing PIDILITIND... ‚úì (Avg size proxy: 20.77)\n",
      "[144/191] Processing PIIND... ‚úì (Avg size proxy: 20.80)\n",
      "[145/191] Processing PNB... ‚úì (Avg size proxy: 21.90)\n",
      "[146/191] Processing POLICYBZR... ‚úì (Avg size proxy: 20.89)\n",
      "[147/191] Processing POLYCAB... ‚úì (Avg size proxy: 21.32)\n",
      "[148/191] Processing POWERGRID... ‚úì (Avg size proxy: 21.87)\n",
      "[149/191] Processing POWERINDIA... ‚úì (Avg size proxy: 19.17)\n",
      "[150/191] Processing PRESTIGE... ‚úì (Avg size proxy: 20.02)\n",
      "[151/191] Processing RECLTD... ‚úì (Avg size proxy: 21.41)\n",
      "[152/191] Processing RELIANCE... ‚úì (Avg size proxy: 23.51)\n",
      "[153/191] Processing RVNL... ‚úì (Avg size proxy: 21.10)\n",
      "[154/191] Processing SAIL... ‚úì (Avg size proxy: 21.64)\n",
      "[155/191] Processing SBICARD... ‚úì (Avg size proxy: 20.85)\n",
      "[156/191] Processing SBILIFE... ‚úì (Avg size proxy: 21.16)\n",
      "[157/191] Processing SBIN... ‚úì (Avg size proxy: 23.02)\n",
      "[158/191] Processing SHREECEM... ‚úì (Avg size proxy: 20.78)\n",
      "[159/191] Processing SHRIRAMFIN... ‚úì (Avg size proxy: 21.37)\n",
      "[160/191] Processing SIEMENS... ‚úì (Avg size proxy: 21.00)\n",
      "[161/191] Processing SOLARINDS... ‚úì (Avg size proxy: 19.68)\n",
      "[162/191] Processing SONACOMS... ‚úì (Avg size proxy: 20.63)\n",
      "[163/191] Processing SRF... ‚úì (Avg size proxy: 21.05)\n",
      "[164/191] Processing SUNPHARMA... ‚úì (Avg size proxy: 21.77)\n",
      "[165/191] Processing SUPREMEIND... ‚úì (Avg size proxy: 19.84)\n",
      "[166/191] Processing SUZLON... ‚úì (Avg size proxy: 21.17)\n",
      "[167/191] Processing TATACOMM... ‚úì (Avg size proxy: 20.74)\n",
      "[168/191] Processing TATACONSUM... ‚úì (Avg size proxy: 21.09)\n",
      "[169/191] Processing TATAELXSI... ‚úì (Avg size proxy: 21.22)\n",
      "[170/191] Processing TATAPOWER... ‚úì (Avg size proxy: 22.14)\n",
      "[171/191] Processing TATASTEEL... ‚úì (Avg size proxy: 22.50)\n",
      "[172/191] Processing TATATECH... ‚úì (Avg size proxy: 21.10)\n",
      "[173/191] Processing TCS... ‚úì (Avg size proxy: 22.76)\n",
      "[174/191] Processing TECHM... ‚úì (Avg size proxy: 21.80)\n",
      "[175/191] Processing TIINDIA... ‚úì (Avg size proxy: 20.45)\n",
      "[176/191] Processing TITAN... ‚úì (Avg size proxy: 21.90)\n",
      "[177/191] Processing TMPV... ‚úì (Avg size proxy: 22.82)\n",
      "[178/191] Processing TORNTPHARM... ‚úì (Avg size proxy: 20.21)\n",
      "[179/191] Processing TORNTPOWER... ‚úì (Avg size proxy: 20.07)\n",
      "[180/191] Processing TRENT... ‚úì (Avg size proxy: 21.05)\n",
      "[181/191] Processing TVSMOTOR... ‚úì (Avg size proxy: 21.29)\n",
      "[182/191] Processing ULTRACEMCO... ‚úì (Avg size proxy: 21.83)\n",
      "[183/191] Processing UNIONBANK... ‚úì (Avg size proxy: 20.84)\n",
      "[184/191] Processing UNITDSPR... ‚úì (Avg size proxy: 20.84)\n",
      "[185/191] Processing UPL... ‚úì (Avg size proxy: 21.26)\n",
      "[186/191] Processing VBL... ‚úì (Avg size proxy: 21.38)\n",
      "[187/191] Processing VEDL... ‚úì (Avg size proxy: 21.89)\n",
      "[188/191] Processing VOLTAS... ‚úì (Avg size proxy: 21.03)\n",
      "[189/191] Processing WIPRO... ‚úì (Avg size proxy: 21.74)\n",
      "[190/191] Processing YESBANK... ‚úì (Avg size proxy: 21.81)\n",
      "[191/191] Processing ZYDUSLIFE... ‚úì (Avg size proxy: 20.43)\n",
      "\n",
      "================================================================================\n",
      "SIZE PROXY CALCULATION COMPLETE\n",
      "================================================================================\n",
      "Total records: 123,725\n",
      "Date range: 2022-01-03 00:00:00 to 2024-08-30 00:00:00\n",
      "\n",
      "‚úÖ Saved to: data/NIFTY200/historical_size_proxy.csv\n",
      "\n",
      "================================================================================\n",
      "SIZE PROXY DISTRIBUTION (Sample Date: 2024-08-30 00:00:00)\n",
      "================================================================================\n",
      "\n",
      "Top 10 (Large Cap):\n",
      "    Symbol   Close   Volume_60d  Size_Proxy\n",
      "  HDFCBANK  818.40 4.900407e+07   24.414765\n",
      "      RVNL  607.40 3.871140e+07   23.880832\n",
      " ICICIBANK 1229.20 1.612372e+07   23.709921\n",
      "  RELIANCE 1509.60 1.250787e+07   23.661469\n",
      "   MAZDOCK 2120.70 8.155748e+06   23.573735\n",
      "      INFY 1943.70 7.833315e+06   23.446245\n",
      "       HAL 4679.95 3.249678e+06   23.445109\n",
      "INDUSTOWER  458.50 3.171926e+07   23.400395\n",
      "   ETERNAL  250.53 5.664541e+07   23.375900\n",
      "      TMPV  765.15 1.854421e+07   23.375740\n",
      "\n",
      "Bottom 10 (Small Cap):\n",
      "    Symbol    Close   Volume_60d  Size_Proxy\n",
      " APLAPOLLO  1462.10 6.476330e+05   20.668708\n",
      "BHARTIHEXA  1252.05 7.380432e+05   20.644295\n",
      "   INDIANB   553.15 1.665558e+06   20.641300\n",
      "    360ONE  1073.00 8.508294e+05   20.632181\n",
      "   PAGEIND 42520.55 2.138160e+04   20.628029\n",
      "MOTILALOFS   720.10 1.188729e+06   20.567786\n",
      "SUPREMEIND  5284.75 1.559509e+05   20.529877\n",
      "BLUESTARCO  1703.90 4.503270e+05   20.458404\n",
      "POWERINDIA 12095.05 6.198783e+04   20.435245\n",
      "BAJAJHLDNG 10057.70 6.129357e+04   20.239524\n"
     ]
    }
   ],
   "source": [
    "# Calculate historical size proxy for neutralization\n",
    "# Using log(price √ó avg_volume_60d) as market cap proxy (correlates ~0.8 with actual market cap)\n",
    "# Rationale: Historical market cap not available; current market cap inappropriate for 2022-2024 data\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CALCULATING HISTORICAL SIZE PROXY FOR ALL STOCKS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Stocks to process: {len(successful_symbols)}\")\n",
    "print(\"Method: log(close √ó rolling_volume_60d)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "size_proxy_data = []\n",
    "\n",
    "for idx, symbol in enumerate(successful_symbols, 1):\n",
    "    print(f\"[{idx}/{len(successful_symbols)}] Processing {symbol}...\", end=\" \")\n",
    "    \n",
    "    try:\n",
    "        # Load stock data\n",
    "        df = pd.read_csv(os.path.join(raw_data_dir, f\"{symbol}.csv\"))\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df = df.sort_values('Date')\n",
    "        \n",
    "        # Calculate 60-day rolling average volume\n",
    "        df['Volume_60d'] = df['Volume'].rolling(window=60, min_periods=30).mean()\n",
    "        \n",
    "        # Calculate size proxy: log(price √ó volume)\n",
    "        df['Size_Proxy'] = np.log(df['Close'] * df['Volume_60d'])\n",
    "        \n",
    "        # Store with date and symbol\n",
    "        df_proxy = df[['Date', 'Close', 'Volume', 'Volume_60d', 'Size_Proxy']].copy()\n",
    "        df_proxy['Symbol'] = symbol\n",
    "        \n",
    "        size_proxy_data.append(df_proxy)\n",
    "        print(f\"‚úì (Avg size proxy: {df['Size_Proxy'].mean():.2f})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error: {e}\")\n",
    "\n",
    "# Combine all data\n",
    "df_size_proxy_all = pd.concat(size_proxy_data, ignore_index=True)\n",
    "\n",
    "# Save to CSV\n",
    "size_proxy_file = \"data/NIFTY200/historical_size_proxy.csv\"\n",
    "df_size_proxy_all.to_csv(size_proxy_file, index=False)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SIZE PROXY CALCULATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total records: {len(df_size_proxy_all):,}\")\n",
    "print(f\"Date range: {df_size_proxy_all['Date'].min()} to {df_size_proxy_all['Date'].max()}\")\n",
    "print(f\"\\n‚úÖ Saved to: {size_proxy_file}\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SIZE PROXY DISTRIBUTION (Sample Date: {})\".format(df_size_proxy_all['Date'].max()))\n",
    "print(\"=\"*80)\n",
    "sample_date = df_size_proxy_all['Date'].max()\n",
    "df_sample = df_size_proxy_all[df_size_proxy_all['Date'] == sample_date].sort_values('Size_Proxy', ascending=False)\n",
    "print(\"\\nTop 10 (Large Cap):\")\n",
    "print(df_sample[['Symbol', 'Close', 'Volume_60d', 'Size_Proxy']].head(10).to_string(index=False))\n",
    "print(\"\\nBottom 10 (Small Cap):\")\n",
    "print(df_sample[['Symbol', 'Close', 'Volume_60d', 'Size_Proxy']].tail(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df5bb72",
   "metadata": {},
   "source": [
    "## 16. Fetch Sector Data from NSE\n",
    "\n",
    "**NEW Task 2.3b:** Get sector classification for neutralization (sector only, not market cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0fbe8d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FETCHING SECTOR DATA FROM NSE\n",
      "================================================================================\n",
      "Stocks to process: 191\n",
      "Est. time: 5-7 minutes (rate limiting)...\n",
      "================================================================================\n",
      "\n",
      "[1/191] 360ONE... ‚úì OTHER\n",
      "[2/191] ABB... ‚úì OTHER\n",
      "[3/191] ABCAPITAL... ‚úì OTHER\n",
      "[4/191] ACC... ‚úì MATERIALS\n",
      "[5/191] ADANIENSOL... ‚úì ENERGY\n",
      "[6/191] ADANIENT... ‚úì OTHER\n",
      "[7/191] ADANIGREEN... ‚úì ENERGY\n",
      "[8/191] ADANIPORTS... ‚úì SERVICES\n",
      "[9/191] ADANIPOWER... ‚úì ENERGY\n",
      "[10/191] ALKEM... ‚úì PHARMA\n",
      "[11/191] AMBUJACEM... ‚úì MATERIALS\n",
      "[12/191] APLAPOLLO... ‚úì OTHER\n",
      "[13/191] APOLLOHOSP... ‚úì OTHER\n",
      "[14/191] ASHOKLEY... ‚úì OTHER\n",
      "[15/191] ASIANPAINT... ‚úì OTHER\n",
      "[16/191] ASTRAL... ‚úì OTHER\n",
      "[17/191] ATGL... ‚úì OTHER\n",
      "[18/191] AUBANK... ‚úì FINANCIAL\n",
      "[19/191] AUROPHARMA... ‚úì PHARMA\n",
      "[20/191] AXISBANK... ‚úì FINANCIAL\n",
      "[21/191] BAJAJ-AUTO... ‚úì OTHER\n",
      "[22/191] BAJAJFINSV... ‚úì OTHER\n",
      "[23/191] BAJAJHLDNG... ‚úì OTHER\n",
      "[24/191] BAJFINANCE... ‚úì FINANCIAL\n",
      "[25/191] BANKBARODA... ‚úì FINANCIAL\n",
      "[26/191] BANKINDIA... ‚úì FINANCIAL\n",
      "[27/191] BDL... ‚úì OTHER\n",
      "[28/191] BEL... ‚úì OTHER\n",
      "[29/191] BHARATFORG... ‚úì OTHER\n",
      "[30/191] BHARTIARTL... ‚úì SERVICES\n",
      "[31/191] BHARTIHEXA... ‚úì SERVICES\n",
      "[32/191] BHEL... ‚úì OTHER\n",
      "[33/191] BIOCON... ‚úì PHARMA\n",
      "[34/191] BLUESTARCO... ‚úì OTHER\n",
      "[35/191] BOSCHLTD... ‚úì OTHER\n",
      "[36/191] BPCL... ‚úì OTHER\n",
      "[37/191] BRITANNIA... ‚úì OTHER\n",
      "[38/191] BSE... ‚úì OTHER\n",
      "[39/191] CANBK... ‚úì FINANCIAL\n",
      "[40/191] CGPOWER... ‚úì OTHER\n",
      "[41/191] CHOLAFIN... ‚úì FINANCIAL\n",
      "[42/191] CIPLA... ‚úì PHARMA\n",
      "[43/191] COALINDIA... ‚úì OTHER\n",
      "[44/191] COCHINSHIP... ‚úì SERVICES\n",
      "[45/191] COFORGE... ‚úì OTHER\n",
      "[46/191] COLPAL... ‚úì OTHER\n",
      "[47/191] CONCOR... ‚úì OTHER\n",
      "[48/191] COROMANDEL... ‚úì OTHER\n",
      "[49/191] CUMMINSIND... ‚úì OTHER\n",
      "[50/191] DABUR... ‚úì OTHER\n",
      "[51/191] DIVISLAB... ‚úì PHARMA\n",
      "[52/191] DIXON... ‚úì OTHER\n",
      "[53/191] DLF... ‚úì OTHER\n",
      "[54/191] DMART... ‚úì OTHER\n",
      "[55/191] DRREDDY... ‚úì PHARMA\n",
      "[56/191] EICHERMOT... ‚úì OTHER\n",
      "[57/191] ETERNAL... ‚úì OTHER\n",
      "[58/191] EXIDEIND... ‚úì OTHER\n",
      "[59/191] FEDERALBNK... ‚úì FINANCIAL\n",
      "[60/191] FORTIS... ‚úì OTHER\n",
      "[61/191] GAIL... ‚úì OTHER\n",
      "[62/191] GLENMARK... ‚úì PHARMA\n",
      "[63/191] GMRAIRPORT... ‚úì SERVICES\n",
      "[64/191] GODFRYPHLP... ‚úì OTHER\n",
      "[65/191] GODREJCP... ‚úì OTHER\n",
      "[66/191] GODREJPROP... ‚úì OTHER\n",
      "[67/191] GRASIM... ‚úì MATERIALS\n",
      "[68/191] HAL... ‚úì OTHER\n",
      "[69/191] HAVELLS... ‚úì OTHER\n",
      "[70/191] HCLTECH... ‚úì OTHER\n",
      "[71/191] HDFCAMC... ‚úì OTHER\n",
      "[72/191] HDFCBANK... ‚úì FINANCIAL\n",
      "[73/191] HDFCLIFE... ‚úì OTHER\n",
      "[74/191] HEROMOTOCO... ‚úì OTHER\n",
      "[75/191] HINDALCO... ‚úì OTHER\n",
      "[76/191] HINDPETRO... ‚úì OTHER\n",
      "[77/191] HINDUNILVR... ‚úì CONSUMER\n",
      "[78/191] HINDZINC... ‚úì OTHER\n",
      "[79/191] HUDCO... ‚úì OTHER\n",
      "[80/191] ICICIBANK... ‚úì FINANCIAL\n",
      "[81/191] ICICIGI... ‚úì OTHER\n",
      "[82/191] IDEA... ‚úì SERVICES\n",
      "[83/191] IDFCFIRSTB... ‚úì FINANCIAL\n",
      "[84/191] IGL... ‚úì OTHER\n",
      "[85/191] INDHOTEL... ‚úì OTHER\n",
      "[86/191] INDIANB... ‚úì FINANCIAL\n",
      "[87/191] INDIGO... ‚úì OTHER\n",
      "[88/191] INDUSINDBK... ‚úì FINANCIAL\n",
      "[89/191] INDUSTOWER... ‚úì OTHER\n",
      "[90/191] INFY... ‚úì OTHER\n",
      "[91/191] IOC... ‚úì OTHER\n",
      "[92/191] IRB... ‚úì OTHER\n",
      "[93/191] IRCTC... ‚úì SERVICES\n",
      "[94/191] IREDA... ‚úì OTHER\n",
      "[95/191] IRFC... ‚úì OTHER\n",
      "[96/191] ITC... ‚úì CONSUMER\n",
      "[97/191] JINDALSTEL... ‚úì OTHER\n",
      "[98/191] JIOFIN... ‚úì OTHER\n",
      "[99/191] JSWENERGY... ‚úì ENERGY\n",
      "[100/191] JSWSTEEL... ‚úì OTHER\n",
      "[101/191] JUBLFOOD... ‚úì OTHER\n",
      "[102/191] KALYANKJIL... ‚úì OTHER\n",
      "[103/191] KEI... ‚úì OTHER\n",
      "[104/191] KOTAKBANK... ‚úì FINANCIAL\n",
      "[105/191] KPITTECH... ‚úì OTHER\n",
      "[106/191] LICHSGFIN... ‚úì FINANCIAL\n",
      "[107/191] LICI... ‚úì OTHER\n",
      "[108/191] LODHA... ‚úì OTHER\n",
      "[109/191] LT... ‚úì OTHER\n",
      "[110/191] LTF... ‚úì OTHER\n",
      "[111/191] LTIM... ‚úì OTHER\n",
      "[112/191] LUPIN... ‚úì PHARMA\n",
      "[113/191] M&M... ‚úì OTHER\n",
      "[114/191] M&MFIN... ‚úì OTHER\n",
      "[115/191] MANKIND... ‚úì PHARMA\n",
      "[116/191] MARICO... ‚úì OTHER\n",
      "[117/191] MARUTI... ‚úì OTHER\n",
      "[118/191] MAXHEALTH... ‚úì OTHER\n",
      "[119/191] MAZDOCK... ‚úì SERVICES\n",
      "[120/191] MFSL... ‚úì OTHER\n",
      "[121/191] MOTHERSON... ‚úì OTHER\n",
      "[122/191] MOTILALOFS... ‚úì OTHER\n",
      "[123/191] MPHASIS... ‚úì OTHER\n",
      "[124/191] MRF... ‚úì OTHER\n",
      "[125/191] MUTHOOTFIN... ‚úì FINANCIAL\n",
      "[126/191] NATIONALUM... ‚úì OTHER\n",
      "[127/191] NAUKRI... ‚úì OTHER\n",
      "[128/191] NESTLEIND... ‚úì OTHER\n",
      "[129/191] NHPC... ‚úì ENERGY\n",
      "[130/191] NMDC... ‚úì OTHER\n",
      "[131/191] NTPC... ‚úì ENERGY\n",
      "[132/191] NYKAA... ‚úì OTHER\n",
      "[133/191] OBEROIRLTY... ‚úì OTHER\n",
      "[134/191] OFSS... ‚úì OTHER\n",
      "[135/191] OIL... ‚úì OTHER\n",
      "[136/191] ONGC... ‚úì OTHER\n",
      "[137/191] PAGEIND... ‚úì OTHER\n",
      "[138/191] PATANJALI... ‚úì OTHER\n",
      "[139/191] PAYTM... ‚úì OTHER\n",
      "[140/191] PERSISTENT... ‚úì OTHER\n",
      "[141/191] PFC... ‚úì OTHER\n",
      "[142/191] PHOENIXLTD... ‚úì OTHER\n",
      "[143/191] PIDILITIND... ‚úì MATERIALS\n",
      "[144/191] PIIND... ‚úì MATERIALS\n",
      "[145/191] PNB... ‚úì FINANCIAL\n",
      "[146/191] POLICYBZR... ‚úì OTHER\n",
      "[147/191] POLYCAB... ‚úì OTHER\n",
      "[148/191] POWERGRID... ‚úì ENERGY\n",
      "[149/191] POWERINDIA... ‚úì OTHER\n",
      "[150/191] PRESTIGE... ‚úì OTHER\n",
      "[151/191] RECLTD... ‚úì OTHER\n",
      "[152/191] RELIANCE... ‚úì OTHER\n",
      "[153/191] RVNL... ‚úì OTHER\n",
      "[154/191] SAIL... ‚úì OTHER\n",
      "[155/191] SBICARD... ‚úì FINANCIAL\n",
      "[156/191] SBILIFE... ‚úì OTHER\n",
      "[157/191] SBIN... ‚úì FINANCIAL\n",
      "[158/191] SHREECEM... ‚úì MATERIALS\n",
      "[159/191] SHRIRAMFIN... ‚úì FINANCIAL\n",
      "[160/191] SIEMENS... ‚úì OTHER\n",
      "[161/191] SOLARINDS... ‚úì OTHER\n",
      "[162/191] SONACOMS... ‚úì OTHER\n",
      "[163/191] SRF... ‚úì MATERIALS\n",
      "[164/191] SUNPHARMA... ‚úì PHARMA\n",
      "[165/191] SUPREMEIND... ‚úì OTHER\n",
      "[166/191] SUZLON... ‚úì OTHER\n",
      "[167/191] TATACOMM... ‚úì SERVICES\n",
      "[168/191] TATACONSUM... ‚úì OTHER\n",
      "[169/191] TATAELXSI... ‚úì OTHER\n",
      "[170/191] TATAPOWER... ‚úì ENERGY\n",
      "[171/191] TATASTEEL... ‚úì OTHER\n",
      "[172/191] TATATECH... ‚úì SERVICES\n",
      "[173/191] TCS... ‚úì OTHER\n",
      "[174/191] TECHM... ‚úì OTHER\n",
      "[175/191] TIINDIA... ‚úì OTHER\n",
      "[176/191] TITAN... ‚úì OTHER\n",
      "[177/191] TMPV... ‚úì OTHER\n",
      "[178/191] TORNTPHARM... ‚úì PHARMA\n",
      "[179/191] TORNTPOWER... ‚úì ENERGY\n",
      "[180/191] TRENT... ‚úì OTHER\n",
      "[181/191] TVSMOTOR... ‚úì OTHER\n",
      "[182/191] ULTRACEMCO... ‚úì MATERIALS\n",
      "[183/191] UNIONBANK... ‚úì FINANCIAL\n",
      "[184/191] UNITDSPR... ‚úì OTHER\n",
      "[185/191] UPL... ‚úì MATERIALS\n",
      "[186/191] VBL... ‚úì OTHER\n",
      "[187/191] VEDL... ‚úì METALS\n",
      "[188/191] VOLTAS... ‚úì OTHER\n",
      "[189/191] WIPRO... ‚úì OTHER\n",
      "[190/191] YESBANK... ‚úì FINANCIAL\n",
      "[191/191] ZYDUSLIFE... ‚úì PHARMA\n",
      "\n",
      "================================================================================\n",
      "SECTOR CLASSIFICATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Major Sector Distribution:\n",
      "major_sector\n",
      "OTHER        126\n",
      "FINANCIAL     22\n",
      "PHARMA        12\n",
      "SERVICES      10\n",
      "ENERGY         9\n",
      "MATERIALS      9\n",
      "CONSUMER       2\n",
      "METALS         1\n",
      "\n",
      "‚úÖ Saved: data/NIFTY200/sector_mapping.csv\n",
      "‚úÖ Saved: stock_info_with_dummies.csv\n",
      "\n",
      "================================================================================\n",
      "‚úÖ TASK 2.3 COMPLETED\n",
      "================================================================================\n",
      "Size proxy: ‚úì Calculated from historical price√óvolume\n",
      "Sector mapping: ‚úì Fetched from NSE\n",
      "Ready for Phase 3: Alpha158 Factor Construction\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Fetch sector data from NSE and map to major categories for neutralization\n",
    "import requests\n",
    "import time\n",
    "\n",
    "def fetch_stock_sector_nse(symbol):\n",
    "    \"\"\"Fetch sector info from NSE API\"\"\"\n",
    "    try:\n",
    "        url = f\"https://www.nseindia.com/api/quote-equity?symbol={symbol}\"\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "            'Accept': 'application/json',\n",
    "            'Referer': 'https://www.nseindia.com/',\n",
    "        }\n",
    "        session = requests.Session()\n",
    "        session.get(\"https://www.nseindia.com\", headers=headers, timeout=10)\n",
    "        time.sleep(1)\n",
    "        response = session.get(url, headers=headers, timeout=10)\n",
    "        data = response.json()\n",
    "        return {\n",
    "            'symbol': symbol,\n",
    "            'company_name': data.get('info', {}).get('companyName', ''),\n",
    "            'sector': data.get('info', {}).get('industry', ''),  # NSE calls it 'industry'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to fetch sector for {symbol}: {e}\")\n",
    "        return {'symbol': symbol, 'company_name': '', 'sector': ''}\n",
    "\n",
    "# Sector mapping to major categories\n",
    "sector_mapping = {\n",
    "    'Financial Services': 'FINANCIAL', 'Bank': 'FINANCIAL', 'Finance': 'FINANCIAL',\n",
    "    'IT - Software': 'IT', 'IT Services & Consulting': 'IT', 'Telecommunication': 'TELECOM',\n",
    "    'Consumer Durables': 'CONSUMER', 'FMCG': 'CONSUMER', 'Retailing': 'CONSUMER',\n",
    "    'Pharma': 'PHARMA', 'Pharmaceuticals': 'PHARMA', 'Healthcare': 'PHARMA',\n",
    "    'Oil & Gas': 'ENERGY', 'Power': 'ENERGY', 'Automobile': 'AUTO',\n",
    "    'Cement': 'MATERIALS', 'Chemicals': 'MATERIALS', 'Metals': 'METALS',\n",
    "    'Media & Entertainment': 'SERVICES', 'Services': 'SERVICES',\n",
    "}\n",
    "\n",
    "def map_to_major_sector(sector_name):\n",
    "    \"\"\"Map detailed sector to major category\"\"\"\n",
    "    if pd.isna(sector_name) or sector_name == '':\n",
    "        return 'OTHER'\n",
    "    for key, value in sector_mapping.items():\n",
    "        if key.lower() in sector_name.lower():\n",
    "            return value\n",
    "    return 'OTHER'\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FETCHING SECTOR DATA FROM NSE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Stocks to process: {len(successful_symbols)}\")\n",
    "print(\"Est. time: 5-7 minutes (rate limiting)...\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "stock_info_list = []\n",
    "for idx, symbol in enumerate(successful_symbols, 1):\n",
    "    print(f\"[{idx}/{len(successful_symbols)}] {symbol}...\", end=\" \")\n",
    "    info = fetch_stock_sector_nse(symbol)\n",
    "    info['major_sector'] = map_to_major_sector(info['sector'])\n",
    "    stock_info_list.append(info)\n",
    "    print(f\"‚úì {info['major_sector']}\")\n",
    "    time.sleep(1.5)\n",
    "\n",
    "df_stock_info = pd.DataFrame(stock_info_list)\n",
    "sector_dummies = pd.get_dummies(df_stock_info['major_sector'], prefix='SECTOR')\n",
    "df_with_dummies = pd.concat([df_stock_info, sector_dummies], axis=1)\n",
    "\n",
    "# Save files\n",
    "sector_mapping_file = \"data/NIFTY200/sector_mapping.csv\"\n",
    "df_stock_info.to_csv(sector_mapping_file, index=False)\n",
    "df_with_dummies.to_csv(\"data/NIFTY200/stock_info_with_dummies.csv\", index=False)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SECTOR CLASSIFICATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nMajor Sector Distribution:\")\n",
    "print(df_stock_info['major_sector'].value_counts().to_string())\n",
    "print(f\"\\n‚úÖ Saved: {sector_mapping_file}\")\n",
    "print(f\"‚úÖ Saved: stock_info_with_dummies.csv\")\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"‚úÖ TASK 2.3 COMPLETED\")\n",
    "print(\"=\"*80)\n",
    "print(\"Size proxy: ‚úì Calculated from historical price√óvolume\")\n",
    "print(\"Sector mapping: ‚úì Fetched from NSE\")\n",
    "print(\"Ready for Phase 3: Alpha158 Factor Construction\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee68d7c",
   "metadata": {},
   "source": [
    "## 17. Validation Gate\n",
    "\n",
    "Brief gate to confirm data quality before proceeding. Run this to assert PASS/FAIL based on completeness, zero-volume, and price-gap checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cac7e7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA QUALITY VALIDATION GATE\n",
      "================================================================================\n",
      "Files analyzed: 191\n",
      "PASS: 191 | WARN: 0 | FAIL: 0\n",
      "\n",
      "‚úÖ Gate Result: PASS - Proceed to Phase 3\n",
      "\n",
      "Note: WARN stocks can remain, but review if many.\n"
     ]
    }
   ],
   "source": [
    "# Data Quality Validation Gate\n",
    "import glob\n",
    "\n",
    "raw_dir = \"data/NIFTY200/raw\"\n",
    "csvs = glob.glob(os.path.join(raw_dir, \"*.csv\"))\n",
    "\n",
    "issues = {\n",
    "    'failed_files': [],\n",
    "    'missing_dates_fail': [],\n",
    "    'zero_volume_warn': [],\n",
    "    'large_gaps_warn': [],\n",
    "}\n",
    "\n",
    "# Criteria\n",
    "MAX_MISSING_PCT = 20.0   # fail if >20% missing trading days\n",
    "MAX_ZERO_VOL_PCT = 10.0  # warn if >10% zero volume\n",
    "MAX_LARGE_GAPS = 5       # warn if >5 days with >20% close-to-close change\n",
    "\n",
    "trading_days_cache = {}\n",
    "\n",
    "def expected_trading_days(start_date, end_date):\n",
    "    key = (start_date, end_date)\n",
    "    if key in trading_days_cache:\n",
    "        return trading_days_cache[key]\n",
    "    rng = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    count = len([d for d in rng if d.weekday() < 5])\n",
    "    trading_days_cache[key] = count\n",
    "    return count\n",
    "\n",
    "passed = 0\n",
    "warned = 0\n",
    "failed = 0\n",
    "\n",
    "for fpath in csvs:\n",
    "    symbol = os.path.basename(fpath).replace('.csv', '')\n",
    "    try:\n",
    "        df = pd.read_csv(fpath)\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df = df.sort_values('Date')\n",
    "        \n",
    "        # Completeness\n",
    "        exp_days = expected_trading_days(df['Date'].min().date(), df['Date'].max().date())\n",
    "        actual_days = len(df)\n",
    "        missing_pct = max(0.0, (exp_days - actual_days) / exp_days * 100.0)\n",
    "        \n",
    "        # Zero volume\n",
    "        zero_vol_pct = (df['Volume'] == 0).sum() / len(df) * 100.0\n",
    "        \n",
    "        # Large price gaps\n",
    "        pct_chg = df['Close'].pct_change().abs()\n",
    "        large_gaps = (pct_chg > 0.20).sum()\n",
    "        \n",
    "        status = 'PASS'\n",
    "        if missing_pct > MAX_MISSING_PCT:\n",
    "            status = 'FAIL'\n",
    "            issues['missing_dates_fail'].append({'symbol': symbol, 'missing_pct': missing_pct})\n",
    "        elif zero_vol_pct > MAX_ZERO_VOL_PCT or large_gaps > MAX_LARGE_GAPS:\n",
    "            status = 'WARN'\n",
    "            if zero_vol_pct > MAX_ZERO_VOL_PCT:\n",
    "                issues['zero_volume_warn'].append({'symbol': symbol, 'zero_vol_pct': zero_vol_pct})\n",
    "            if large_gaps > MAX_LARGE_GAPS:\n",
    "                issues['large_gaps_warn'].append({'symbol': symbol, 'large_gaps': int(large_gaps)})\n",
    "        \n",
    "        if status == 'PASS':\n",
    "            passed += 1\n",
    "        elif status == 'WARN':\n",
    "            warned += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "    except Exception as e:\n",
    "        issues['failed_files'].append({'symbol': symbol, 'error': str(e)})\n",
    "        failed += 1\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATA QUALITY VALIDATION GATE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Files analyzed: {len(csvs)}\")\n",
    "print(f\"PASS: {passed} | WARN: {warned} | FAIL: {failed}\")\n",
    "\n",
    "if failed == 0:\n",
    "    print(\"\\n‚úÖ Gate Result: PASS - Proceed to Phase 3\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Gate Result: FAIL - Fix issues before proceeding\")\n",
    "\n",
    "# Show top issues\n",
    "if issues['missing_dates_fail']:\n",
    "    print(\"\\nFailures (Missing >20% trading days):\")\n",
    "    for item in sorted(issues['missing_dates_fail'], key=lambda x: -x['missing_pct'])[:10]:\n",
    "        print(f\"  - {item['symbol']}: missing {item['missing_pct']:.1f}%\")\n",
    "\n",
    "if issues['zero_volume_warn']:\n",
    "    print(\"\\nWarnings (Zero volume >10%):\")\n",
    "    for item in sorted(issues['zero_volume_warn'], key=lambda x: -x['zero_vol_pct'])[:10]:\n",
    "        print(f\"  - {item['symbol']}: zero volume {item['zero_vol_pct']:.1f}%\")\n",
    "\n",
    "if issues['large_gaps_warn']:\n",
    "    print(\"\\nWarnings (Large price gaps >5 days):\")\n",
    "    for item in sorted(issues['large_gaps_warn'], key=lambda x: -x['large_gaps'])[:10]:\n",
    "        print(f\"  - {item['symbol']}: {item['large_gaps']} large gaps\")\n",
    "\n",
    "print(\"\\nNote: WARN stocks can remain, but review if many.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfb797b",
   "metadata": {},
   "source": [
    "## 18. Size Proxy Pivot for Neutralization\n",
    "\n",
    "Brief note: we use log(close √ó rolling 60d volume) as size proxy. The pivoted matrix matches the algorithm‚Äôs expected log-size input shape (dates √ó symbols)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "292a836b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SIZE PROXY PIVOT CREATED\n",
      "================================================================================\n",
      "Shape: (631, 191) (dates √ó symbols)\n",
      "Columns sample: ['360ONE', 'ABB', 'ABCAPITAL', 'ACC', 'ADANIENSOL', 'ADANIENT', 'ADANIGREEN', 'ADANIPORTS']\n",
      "Date range: 2022-02-14 to 2024-08-30\n",
      "\n",
      "‚úÖ Saved to: data/NIFTY200/size_proxy_pivot.csv\n",
      "\n",
      "Note: This matrix can be used as log-size input in neutralization (same shape as factor matrices).\n"
     ]
    }
   ],
   "source": [
    "# Pivot historical size proxy to dates √ó symbols matrix\n",
    "size_proxy_file = \"data/NIFTY200/historical_size_proxy.csv\"\n",
    "\n",
    "try:\n",
    "    df_proxy = pd.read_csv(size_proxy_file)\n",
    "    df_proxy['Date'] = pd.to_datetime(df_proxy['Date'])\n",
    "    \n",
    "    # Pivot: index = Date, columns = Symbol, values = Size_Proxy\n",
    "    pivot = df_proxy.pivot_table(index='Date', columns='Symbol', values='Size_Proxy')\n",
    "    pivot = pivot.sort_index()\n",
    "    \n",
    "    out_file = \"data/NIFTY200/size_proxy_pivot.csv\"\n",
    "    pivot.to_csv(out_file)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"SIZE PROXY PIVOT CREATED\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Shape: {pivot.shape} (dates √ó symbols)\")\n",
    "    print(f\"Columns sample: {list(pivot.columns[:8])}\")\n",
    "    print(f\"Date range: {pivot.index.min().date()} to {pivot.index.max().date()}\")\n",
    "    print(f\"\\n‚úÖ Saved to: {out_file}\")\n",
    "    print(\"\\nNote: This matrix can be used as log-size input in neutralization (same shape as factor matrices).\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating pivot: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phi3-k8s-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
